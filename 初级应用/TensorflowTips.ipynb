{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tensorflow快速入门二\n",
    "## 知识点总结\n",
    "* 梯度下降方法总结\n",
    "* 模型保存和恢复\n",
    "# 梯度下降方法总结\n",
    "* Tensorflow直接计算，满足公式$\\theta=\\theta-\\eta \\nabla_\\theta M S E(\\theta)$ \n",
    "* tf.random_uniform()：在图中创建一个包含随机值的节点，类似于NumPy中的random()函数\n",
    "* tf.assign()：创建一个将新值赋给变量的一个节点，为variable更新值\n",
    "---\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77a3a57e908011c7"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "517fe97af095f25c"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'feature_names', 'DESCR'])\n",
      "['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n",
      "[4.526 3.585 3.521 ... 0.923 0.847 0.894]\n",
      ".. _california_housing_dataset:\n",
      "\n",
      "California Housing dataset\n",
      "--------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 20640\n",
      "\n",
      "    :Number of Attributes: 8 numeric, predictive attributes and the target\n",
      "\n",
      "    :Attribute Information:\n",
      "        - MedInc        median income in block\n",
      "        - HouseAge      median house age in block\n",
      "        - AveRooms      average number of rooms\n",
      "        - AveBedrms     average number of bedrooms\n",
      "        - Population    block population\n",
      "        - AveOccup      average house occupancy\n",
      "        - Latitude      house block latitude\n",
      "        - Longitude     house block longitude\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "This dataset was obtained from the StatLib repository.\n",
      "http://lib.stat.cmu.edu/datasets/\n",
      "\n",
      "The target variable is the median house value for California districts.\n",
      "\n",
      "This dataset was derived from the 1990 U.S. census, using one row per census\n",
      "block group. A block group is the smallest geographical unit for which the U.S.\n",
      "Census Bureau publishes sample data (a block group typically has a population\n",
      "of 600 to 3,000 people).\n",
      "\n",
      "It can be downloaded/loaded using the\n",
      ":func:`sklearn.datasets.fetch_california_housing` function.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
      "      Statistics and Probability Letters, 33 (1997) 291-297\n"
     ]
    }
   ],
   "source": [
    "#--coding--:utf-8\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# 获取数据\n",
    "housing = fetch_california_housing()\n",
    "m,n = housing.data.shape\n",
    "print(housing.keys())\n",
    "print(housing.feature_names)\n",
    "print(housing.target)\n",
    "print(housing.DESCR)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T10:58:49.552937200Z",
     "start_time": "2024-02-28T10:58:49.499082700Z"
    }
   },
   "id": "2db05ab32e768c94"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "# 关于警告\n",
    "有点警告，但是不要紧，因为我也不确定具体的版本是多少，只能大概接近。\n",
    "\n",
    "关于代码：\n",
    "```python\n",
    "housing_data_plus_bias = np.c_[np.ones((m,1)), housing.data]\n",
    "scaled_data = scaler.fit_transform(housing.data)\n",
    "data = np.c_[np.ones((m,1)),scaled_data]\n",
    "```\n",
    "文心一言的解释：\n",
    "\n",
    "\n",
    "---\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ab58f74476bcd3a"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 1.        ,  2.34476576,  0.98214266, ..., -0.04959654,\n         1.05254828, -1.32783522],\n       [ 1.        ,  2.33223796, -0.60701891, ..., -0.09251223,\n         1.04318455, -1.32284391],\n       [ 1.        ,  1.7826994 ,  1.85618152, ..., -0.02584253,\n         1.03850269, -1.33282653],\n       ...,\n       [ 1.        , -1.14259331, -0.92485123, ..., -0.0717345 ,\n         1.77823747, -0.8237132 ],\n       [ 1.        , -1.05458292, -0.84539315, ..., -0.09122515,\n         1.77823747, -0.87362627],\n       [ 1.        , -0.78012947, -1.00430931, ..., -0.04368215,\n         1.75014627, -0.83369581]])"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_data_plus_bias = np.c_[np.ones((m,1)), housing.data]\n",
    "scaled_data = scaler.fit_transform(housing.data)\n",
    "data = np.c_[np.ones((m,1)),scaled_data]\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T10:58:49.553934300Z",
     "start_time": "2024-02-28T10:58:49.531615500Z"
    }
   },
   "id": "731b17a61ebfcf7e"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 15.371014\n",
      "Epoch 100 MSE = 15.371014\n",
      "Epoch 200 MSE = 15.371014\n",
      "Epoch 300 MSE = 15.371014\n",
      "Epoch 400 MSE = 15.371014\n",
      "Epoch 500 MSE = 15.371014\n",
      "Epoch 600 MSE = 15.371014\n",
      "Epoch 700 MSE = 15.371014\n",
      "Epoch 800 MSE = 15.371014\n",
      "Epoch 900 MSE = 15.371014\n",
      "best theta: [[-0.9993887 ]\n",
      " [-0.37894058]\n",
      " [-0.79117846]\n",
      " [ 0.42145848]\n",
      " [-0.7427218 ]\n",
      " [ 0.7037499 ]\n",
      " [ 0.63889766]\n",
      " [-0.92098784]\n",
      " [ 0.48915935]]\n"
     ]
    }
   ],
   "source": [
    "# 设置参数\n",
    "n_epoch = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "# 设置placeholder\n",
    "X = tf.constant(data,dtype = tf.float32,name = \"X\")\n",
    "y = tf.constant(housing.target.reshape(-1,1),dtype=tf.float32,name='y')\n",
    "\n",
    "theta = tf.Variable(tf.random_uniform([n+1, 1], -1, 1),name='theta')\n",
    "y_pred = tf.matmul(X,theta,name='prediction')\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error),name='mse') #Mean Squared Error \n",
    "\"\"\"均方误差\"\"\"\n",
    "\n",
    "# 计算梯度公式\n",
    "\"\"\"\n",
    "麻烦在梯度公式需要我们自己手动去写，\n",
    "现在只有一层，之后说不定有几千层，所以我们肯定不能自己手写，而且还不一定准确。\n",
    "\"\"\"\n",
    "gradient = 2/m * tf.matmul(tf.transpose(X),error)\n",
    "training_op = tf.assign(theta,theta - learning_rate * gradient)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epoch):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\",epoch, \"MSE =\", mse.eval())\n",
    "\n",
    "    print('best theta:',theta.eval())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T10:58:49.638590800Z",
     "start_time": "2024-02-28T10:58:49.547947600Z"
    }
   },
   "id": "7cb1a02cad5cf2ce"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# 自动求导autodiff \n",
    "手动求导方法有两个缺陷：\n",
    "* 深度神经网络中公式较长\n",
    "* 计算效率较低 可以采取以下命令进行自动求导：\n",
    "gradients = tf.gradients(mse,[theta])[0]\n",
    "gradient有两个参数：\n",
    "* op，损失函数，这里是mse\n",
    "* variable lists，变量列表，这里是theta值\n",
    "**这就已经很方便了，但还有更方便的方法**\n",
    "\n",
    "---\n",
    "# 使用Optimizer\n",
    "可以使用各种优化器,如梯度下降优化器\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "还有很多其他优化器，例如收敛更快的MomentumOptimizer优化器等\n",
    "# 梯度下降中传输数据的方式\n",
    "* mini-batchbatch\n",
    "* 方法：使用占位符placeholder\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c411539d0ad74f77"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\huanyizhiyuan\\AppData\\Local\\Temp\\ipykernel_32524\\1842848948.py:2: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "[[6. 7. 8.]]\n",
      "[[ 9. 10. 11.]\n",
      " [12. 13. 14.]]\n"
     ]
    }
   ],
   "source": [
    "# 传统方式\n",
    "A = tf.placeholder(tf.float32,shape=(None,3))\n",
    "B = A + 5\n",
    "with tf.Session() as sess:\n",
    "    test_b_1 = B.eval(feed_dict={A:[[1,2,3]]})\n",
    "    test_b_2 = B.eval(feed_dict={A:[[4,5,6],[7,8,9]]})\n",
    "print(test_b_1)  \n",
    "print(test_b_2) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T10:59:58.697593Z",
     "start_time": "2024-02-28T10:59:58.647231300Z"
    }
   },
   "id": "176ff88cc8bfbe42"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\huanyizhiyuan\\AppData\\Local\\Temp\\ipykernel_32524\\966860098.py:15: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\n",
      "epoch: 0 batch_id: 0 Batch loss: 2.1672046\n",
      "epoch: 0 batch_id: 1 Batch loss: 2.295659\n",
      "epoch: 0 batch_id: 2 Batch loss: 1.874109\n",
      "epoch: 0 batch_id: 3 Batch loss: 2.2121804\n",
      "epoch: 0 batch_id: 4 Batch loss: 2.167641\n",
      "epoch: 0 batch_id: 5 Batch loss: 1.7245495\n",
      "epoch: 0 batch_id: 6 Batch loss: 2.0318623\n",
      "epoch: 0 batch_id: 7 Batch loss: 1.5136069\n",
      "epoch: 0 batch_id: 8 Batch loss: 1.6046904\n",
      "epoch: 0 batch_id: 9 Batch loss: 1.3764426\n",
      "epoch: 0 batch_id: 10 Batch loss: 1.3385471\n",
      "epoch: 0 batch_id: 11 Batch loss: 1.7322328\n",
      "epoch: 0 batch_id: 12 Batch loss: 1.4998823\n",
      "epoch: 0 batch_id: 13 Batch loss: 1.9056491\n",
      "epoch: 0 batch_id: 14 Batch loss: 3.0762713\n",
      "epoch: 0 batch_id: 15 Batch loss: 1.2916967\n",
      "epoch: 0 batch_id: 16 Batch loss: 1.5571638\n",
      "epoch: 0 batch_id: 17 Batch loss: 1.598186\n",
      "epoch: 0 batch_id: 18 Batch loss: 1.3730277\n",
      "epoch: 0 batch_id: 19 Batch loss: 1.1644965\n",
      "epoch: 0 batch_id: 20 Batch loss: 1.1785976\n",
      "epoch: 0 batch_id: 21 Batch loss: 1.576241\n",
      "epoch: 0 batch_id: 22 Batch loss: 1.4259119\n",
      "epoch: 0 batch_id: 23 Batch loss: 1.4336585\n",
      "epoch: 0 batch_id: 24 Batch loss: 1.2611841\n",
      "epoch: 0 batch_id: 25 Batch loss: 1.4847397\n",
      "epoch: 0 batch_id: 26 Batch loss: 1.1937325\n",
      "epoch: 0 batch_id: 27 Batch loss: 1.2509804\n",
      "epoch: 0 batch_id: 28 Batch loss: 1.1227579\n",
      "epoch: 0 batch_id: 29 Batch loss: 1.2972305\n",
      "epoch: 0 batch_id: 30 Batch loss: 0.82125306\n",
      "epoch: 0 batch_id: 31 Batch loss: 1.0552242\n",
      "epoch: 0 batch_id: 32 Batch loss: 0.9571347\n",
      "epoch: 0 batch_id: 33 Batch loss: 0.7215873\n",
      "epoch: 0 batch_id: 34 Batch loss: 1.4499353\n",
      "epoch: 0 batch_id: 35 Batch loss: 1.1247096\n",
      "epoch: 0 batch_id: 36 Batch loss: 1.1818776\n",
      "epoch: 0 batch_id: 37 Batch loss: 0.76467866\n",
      "epoch: 0 batch_id: 38 Batch loss: 0.99780595\n",
      "epoch: 0 batch_id: 39 Batch loss: 1.0460093\n",
      "epoch: 0 batch_id: 40 Batch loss: 1.0013143\n",
      "epoch: 0 batch_id: 41 Batch loss: 0.784319\n",
      "epoch: 0 batch_id: 42 Batch loss: 0.9435274\n",
      "epoch: 0 batch_id: 43 Batch loss: 0.82936156\n",
      "epoch: 0 batch_id: 44 Batch loss: 1.1125345\n",
      "epoch: 0 batch_id: 45 Batch loss: 0.6104064\n",
      "epoch: 0 batch_id: 46 Batch loss: 0.8859728\n",
      "epoch: 0 batch_id: 47 Batch loss: 0.4898642\n",
      "epoch: 0 batch_id: 48 Batch loss: 0.9092212\n",
      "epoch: 0 batch_id: 49 Batch loss: 0.5732553\n",
      "epoch: 0 batch_id: 50 Batch loss: 0.6138911\n",
      "epoch: 0 batch_id: 51 Batch loss: 0.8413237\n",
      "epoch: 0 batch_id: 52 Batch loss: 0.72109926\n",
      "epoch: 0 batch_id: 53 Batch loss: 0.81451416\n",
      "epoch: 0 batch_id: 54 Batch loss: 0.68089575\n",
      "epoch: 0 batch_id: 55 Batch loss: 0.7564277\n",
      "epoch: 0 batch_id: 56 Batch loss: 0.41313285\n",
      "epoch: 0 batch_id: 57 Batch loss: 0.80073553\n",
      "epoch: 0 batch_id: 58 Batch loss: 1.0787388\n",
      "epoch: 0 batch_id: 59 Batch loss: 0.8186071\n",
      "epoch: 0 batch_id: 60 Batch loss: 0.7204998\n",
      "epoch: 0 batch_id: 61 Batch loss: 0.92680025\n",
      "epoch: 0 batch_id: 62 Batch loss: 0.5101226\n",
      "epoch: 0 batch_id: 63 Batch loss: 0.50260293\n",
      "epoch: 0 batch_id: 64 Batch loss: 0.8276913\n",
      "epoch: 0 batch_id: 65 Batch loss: 0.9545834\n",
      "epoch: 0 batch_id: 66 Batch loss: 0.80202603\n",
      "epoch: 0 batch_id: 67 Batch loss: 0.67271215\n",
      "epoch: 0 batch_id: 68 Batch loss: 2.9928567\n",
      "epoch: 0 batch_id: 69 Batch loss: 0.6856038\n",
      "epoch: 0 batch_id: 70 Batch loss: 0.62334716\n",
      "epoch: 0 batch_id: 71 Batch loss: 0.91120476\n",
      "epoch: 0 batch_id: 72 Batch loss: 0.6585442\n",
      "epoch: 0 batch_id: 73 Batch loss: 0.9744072\n",
      "epoch: 0 batch_id: 74 Batch loss: 0.7742579\n",
      "epoch: 0 batch_id: 75 Batch loss: 0.6426654\n",
      "epoch: 0 batch_id: 76 Batch loss: 0.8751979\n",
      "epoch: 0 batch_id: 77 Batch loss: 0.6103282\n",
      "epoch: 0 batch_id: 78 Batch loss: 0.52216816\n",
      "epoch: 0 batch_id: 79 Batch loss: 0.6579976\n",
      "epoch: 0 batch_id: 80 Batch loss: 0.6485384\n",
      "epoch: 0 batch_id: 81 Batch loss: 0.49898323\n",
      "epoch: 0 batch_id: 82 Batch loss: 0.43097228\n",
      "epoch: 0 batch_id: 83 Batch loss: 0.5906239\n",
      "epoch: 0 batch_id: 84 Batch loss: 0.6949865\n",
      "epoch: 0 batch_id: 85 Batch loss: 0.55648214\n",
      "epoch: 0 batch_id: 86 Batch loss: 0.508226\n",
      "epoch: 0 batch_id: 87 Batch loss: 0.6279615\n",
      "epoch: 0 batch_id: 88 Batch loss: 0.5880988\n",
      "epoch: 0 batch_id: 89 Batch loss: 0.68942225\n",
      "epoch: 0 batch_id: 90 Batch loss: 0.6485301\n",
      "epoch: 0 batch_id: 91 Batch loss: 0.70109934\n",
      "epoch: 0 batch_id: 92 Batch loss: 0.8923778\n",
      "epoch: 0 batch_id: 93 Batch loss: 0.49801528\n",
      "epoch: 0 batch_id: 94 Batch loss: 0.48743966\n",
      "epoch: 0 batch_id: 95 Batch loss: 0.558912\n",
      "epoch: 0 batch_id: 96 Batch loss: 0.48256263\n",
      "epoch: 0 batch_id: 97 Batch loss: 0.5622009\n",
      "epoch: 0 batch_id: 98 Batch loss: 0.74889356\n",
      "epoch: 0 batch_id: 99 Batch loss: 0.5001389\n",
      "epoch: 0 batch_id: 100 Batch loss: 0.7956856\n",
      "epoch: 0 batch_id: 101 Batch loss: 0.80084395\n",
      "epoch: 0 batch_id: 102 Batch loss: 0.6526503\n",
      "epoch: 0 batch_id: 103 Batch loss: 0.99844605\n",
      "epoch: 0 batch_id: 104 Batch loss: 0.6220297\n",
      "epoch: 0 batch_id: 105 Batch loss: 0.55140287\n",
      "epoch: 0 batch_id: 106 Batch loss: 0.660471\n",
      "epoch: 0 batch_id: 107 Batch loss: 0.62981\n",
      "epoch: 0 batch_id: 108 Batch loss: 0.42868996\n",
      "epoch: 0 batch_id: 109 Batch loss: 0.6170795\n",
      "epoch: 0 batch_id: 110 Batch loss: 0.82903135\n",
      "epoch: 0 batch_id: 111 Batch loss: 0.9344252\n",
      "epoch: 0 batch_id: 112 Batch loss: 0.70588875\n",
      "epoch: 0 batch_id: 113 Batch loss: 0.37962767\n",
      "epoch: 0 batch_id: 114 Batch loss: 0.7631793\n",
      "epoch: 0 batch_id: 115 Batch loss: 0.7156675\n",
      "epoch: 0 batch_id: 116 Batch loss: 0.46273124\n",
      "epoch: 0 batch_id: 117 Batch loss: 0.6482007\n",
      "epoch: 0 batch_id: 118 Batch loss: 0.60012215\n",
      "epoch: 0 batch_id: 119 Batch loss: 0.40776208\n",
      "epoch: 0 batch_id: 120 Batch loss: 0.4982826\n",
      "epoch: 0 batch_id: 121 Batch loss: 0.48016846\n",
      "epoch: 0 batch_id: 122 Batch loss: 0.52411824\n",
      "epoch: 0 batch_id: 123 Batch loss: 0.4921044\n",
      "epoch: 0 batch_id: 124 Batch loss: 0.45854026\n",
      "epoch: 0 batch_id: 125 Batch loss: 0.36903068\n",
      "epoch: 0 batch_id: 126 Batch loss: 0.61913115\n",
      "epoch: 0 batch_id: 127 Batch loss: 0.3797666\n",
      "epoch: 0 batch_id: 128 Batch loss: 0.45355907\n",
      "epoch: 0 batch_id: 129 Batch loss: 0.72294354\n",
      "epoch: 0 batch_id: 130 Batch loss: 0.62233526\n",
      "epoch: 0 batch_id: 131 Batch loss: 0.53545594\n",
      "epoch: 0 batch_id: 132 Batch loss: 0.698982\n",
      "epoch: 0 batch_id: 133 Batch loss: 0.6234777\n",
      "epoch: 0 batch_id: 134 Batch loss: 0.58243924\n",
      "epoch: 0 batch_id: 135 Batch loss: 0.49832892\n",
      "epoch: 0 batch_id: 136 Batch loss: 0.507825\n",
      "epoch: 0 batch_id: 137 Batch loss: 0.48021522\n",
      "epoch: 0 batch_id: 138 Batch loss: 0.63356835\n",
      "epoch: 0 batch_id: 139 Batch loss: 0.5370758\n",
      "epoch: 0 batch_id: 140 Batch loss: 0.33848\n",
      "epoch: 0 batch_id: 141 Batch loss: 0.5365341\n",
      "epoch: 0 batch_id: 142 Batch loss: 0.4522192\n",
      "epoch: 0 batch_id: 143 Batch loss: 0.5850051\n",
      "epoch: 0 batch_id: 144 Batch loss: 0.573858\n",
      "epoch: 0 batch_id: 145 Batch loss: 0.6102801\n",
      "epoch: 0 batch_id: 146 Batch loss: 0.91598016\n",
      "epoch: 0 batch_id: 147 Batch loss: 0.48514703\n",
      "epoch: 0 batch_id: 148 Batch loss: 0.41607916\n",
      "epoch: 0 batch_id: 149 Batch loss: 0.62040544\n",
      "epoch: 0 batch_id: 150 Batch loss: 0.6620631\n",
      "epoch: 0 batch_id: 151 Batch loss: 0.7080032\n",
      "epoch: 0 batch_id: 152 Batch loss: 0.6137735\n",
      "epoch: 0 batch_id: 153 Batch loss: 0.5625401\n",
      "epoch: 0 batch_id: 154 Batch loss: 0.58069277\n",
      "epoch: 0 batch_id: 155 Batch loss: 0.6507666\n",
      "epoch: 0 batch_id: 156 Batch loss: 0.5379826\n",
      "epoch: 0 batch_id: 157 Batch loss: 0.48604012\n",
      "epoch: 0 batch_id: 158 Batch loss: 0.6648266\n",
      "epoch: 0 batch_id: 159 Batch loss: 0.5922148\n",
      "epoch: 0 batch_id: 160 Batch loss: 0.4267631\n",
      "epoch: 0 batch_id: 161 Batch loss: 0.47565505\n",
      "epoch: 0 batch_id: 162 Batch loss: 0.6988977\n",
      "epoch: 0 batch_id: 163 Batch loss: 0.779425\n",
      "epoch: 0 batch_id: 164 Batch loss: 0.8383062\n",
      "epoch: 0 batch_id: 165 Batch loss: 0.46808872\n",
      "epoch: 0 batch_id: 166 Batch loss: 0.5153188\n",
      "epoch: 0 batch_id: 167 Batch loss: 0.53328824\n",
      "epoch: 0 batch_id: 168 Batch loss: 0.5647166\n",
      "epoch: 0 batch_id: 169 Batch loss: 0.34861624\n",
      "epoch: 0 batch_id: 170 Batch loss: 0.40503472\n",
      "epoch: 0 batch_id: 171 Batch loss: 0.63356704\n",
      "epoch: 0 batch_id: 172 Batch loss: 0.5648425\n",
      "epoch: 0 batch_id: 173 Batch loss: 0.464232\n",
      "epoch: 0 batch_id: 174 Batch loss: 0.6498482\n",
      "epoch: 0 batch_id: 175 Batch loss: 0.4954551\n",
      "epoch: 0 batch_id: 176 Batch loss: 0.4477538\n",
      "epoch: 0 batch_id: 177 Batch loss: 0.5217755\n",
      "epoch: 0 batch_id: 178 Batch loss: 0.4477166\n",
      "epoch: 0 batch_id: 179 Batch loss: 0.37924206\n",
      "epoch: 0 batch_id: 180 Batch loss: 0.8637709\n",
      "epoch: 0 batch_id: 181 Batch loss: 0.7067162\n",
      "epoch: 0 batch_id: 182 Batch loss: 0.45683166\n",
      "epoch: 0 batch_id: 183 Batch loss: 0.54429954\n",
      "epoch: 0 batch_id: 184 Batch loss: 0.721996\n",
      "epoch: 0 batch_id: 185 Batch loss: 0.6003588\n",
      "epoch: 0 batch_id: 186 Batch loss: 0.61476725\n",
      "epoch: 0 batch_id: 187 Batch loss: 0.7391876\n",
      "epoch: 0 batch_id: 188 Batch loss: 0.81524295\n",
      "epoch: 0 batch_id: 189 Batch loss: 0.6050756\n",
      "epoch: 0 batch_id: 190 Batch loss: 0.5068627\n",
      "epoch: 0 batch_id: 191 Batch loss: 0.41507104\n",
      "epoch: 0 batch_id: 192 Batch loss: 0.5305283\n",
      "epoch: 0 batch_id: 193 Batch loss: 0.65801066\n",
      "epoch: 0 batch_id: 194 Batch loss: 0.75988114\n",
      "epoch: 0 batch_id: 195 Batch loss: 0.44991416\n",
      "epoch: 0 batch_id: 196 Batch loss: 0.7162283\n",
      "epoch: 0 batch_id: 197 Batch loss: 0.5700762\n",
      "epoch: 0 batch_id: 198 Batch loss: 0.7161691\n",
      "epoch: 0 batch_id: 199 Batch loss: 0.5129765\n",
      "epoch: 0 batch_id: 200 Batch loss: 0.45167762\n",
      "epoch: 0 batch_id: 201 Batch loss: 0.5767804\n",
      "epoch: 0 batch_id: 202 Batch loss: 0.83825713\n",
      "epoch: 0 batch_id: 203 Batch loss: 0.54183465\n",
      "epoch: 0 batch_id: 204 Batch loss: 0.57411736\n",
      "epoch: 0 batch_id: 205 Batch loss: 0.3628704\n",
      "epoch: 0 batch_id: 206 Batch loss: 0.502484\n",
      "epoch: 10 batch_id: 0 Batch loss: 0.5051014\n",
      "epoch: 10 batch_id: 1 Batch loss: 0.5390079\n",
      "epoch: 10 batch_id: 2 Batch loss: 0.7699985\n",
      "epoch: 10 batch_id: 3 Batch loss: 0.45973587\n",
      "epoch: 10 batch_id: 4 Batch loss: 0.5360129\n",
      "epoch: 10 batch_id: 5 Batch loss: 0.6091374\n",
      "epoch: 10 batch_id: 6 Batch loss: 0.4848224\n",
      "epoch: 10 batch_id: 7 Batch loss: 0.64357764\n",
      "epoch: 10 batch_id: 8 Batch loss: 0.39952487\n",
      "epoch: 10 batch_id: 9 Batch loss: 0.5636877\n",
      "epoch: 10 batch_id: 10 Batch loss: 0.5957983\n",
      "epoch: 10 batch_id: 11 Batch loss: 0.55822176\n",
      "epoch: 10 batch_id: 12 Batch loss: 0.60682464\n",
      "epoch: 10 batch_id: 13 Batch loss: 0.7249879\n",
      "epoch: 10 batch_id: 14 Batch loss: 0.436547\n",
      "epoch: 10 batch_id: 15 Batch loss: 0.4193787\n",
      "epoch: 10 batch_id: 16 Batch loss: 0.44316062\n",
      "epoch: 10 batch_id: 17 Batch loss: 0.47150213\n",
      "epoch: 10 batch_id: 18 Batch loss: 0.5420638\n",
      "epoch: 10 batch_id: 19 Batch loss: 0.7725069\n",
      "epoch: 10 batch_id: 20 Batch loss: 0.72334397\n",
      "epoch: 10 batch_id: 21 Batch loss: 0.48796403\n",
      "epoch: 10 batch_id: 22 Batch loss: 0.51697886\n",
      "epoch: 10 batch_id: 23 Batch loss: 0.83265597\n",
      "epoch: 10 batch_id: 24 Batch loss: 0.46266755\n",
      "epoch: 10 batch_id: 25 Batch loss: 0.5021713\n",
      "epoch: 10 batch_id: 26 Batch loss: 0.39823487\n",
      "epoch: 10 batch_id: 27 Batch loss: 0.5411586\n",
      "epoch: 10 batch_id: 28 Batch loss: 0.5010817\n",
      "epoch: 10 batch_id: 29 Batch loss: 0.73477966\n",
      "epoch: 10 batch_id: 30 Batch loss: 0.5478465\n",
      "epoch: 10 batch_id: 31 Batch loss: 0.505095\n",
      "epoch: 10 batch_id: 32 Batch loss: 0.4772895\n",
      "epoch: 10 batch_id: 33 Batch loss: 0.620199\n",
      "epoch: 10 batch_id: 34 Batch loss: 0.48860186\n",
      "epoch: 10 batch_id: 35 Batch loss: 0.48484913\n",
      "epoch: 10 batch_id: 36 Batch loss: 0.45456392\n",
      "epoch: 10 batch_id: 37 Batch loss: 0.40062463\n",
      "epoch: 10 batch_id: 38 Batch loss: 0.43476355\n",
      "epoch: 10 batch_id: 39 Batch loss: 0.50642824\n",
      "epoch: 10 batch_id: 40 Batch loss: 0.43764943\n",
      "epoch: 10 batch_id: 41 Batch loss: 0.46811163\n",
      "epoch: 10 batch_id: 42 Batch loss: 0.74329597\n",
      "epoch: 10 batch_id: 43 Batch loss: 0.38543102\n",
      "epoch: 10 batch_id: 44 Batch loss: 0.6400278\n",
      "epoch: 10 batch_id: 45 Batch loss: 0.4899668\n",
      "epoch: 10 batch_id: 46 Batch loss: 0.39218536\n",
      "epoch: 10 batch_id: 47 Batch loss: 0.82012236\n",
      "epoch: 10 batch_id: 48 Batch loss: 0.44874465\n",
      "epoch: 10 batch_id: 49 Batch loss: 0.7113634\n",
      "epoch: 10 batch_id: 50 Batch loss: 0.56050074\n",
      "epoch: 10 batch_id: 51 Batch loss: 0.5093424\n",
      "epoch: 10 batch_id: 52 Batch loss: 0.56201303\n",
      "epoch: 10 batch_id: 53 Batch loss: 0.6693562\n",
      "epoch: 10 batch_id: 54 Batch loss: 0.32174754\n",
      "epoch: 10 batch_id: 55 Batch loss: 0.45630014\n",
      "epoch: 10 batch_id: 56 Batch loss: 0.55090296\n",
      "epoch: 10 batch_id: 57 Batch loss: 0.50637335\n",
      "epoch: 10 batch_id: 58 Batch loss: 0.56452066\n",
      "epoch: 10 batch_id: 59 Batch loss: 0.4233708\n",
      "epoch: 10 batch_id: 60 Batch loss: 0.35257813\n",
      "epoch: 10 batch_id: 61 Batch loss: 0.4614594\n",
      "epoch: 10 batch_id: 62 Batch loss: 0.4281748\n",
      "epoch: 10 batch_id: 63 Batch loss: 0.5340724\n",
      "epoch: 10 batch_id: 64 Batch loss: 0.5962833\n",
      "epoch: 10 batch_id: 65 Batch loss: 0.6197733\n",
      "epoch: 10 batch_id: 66 Batch loss: 0.63037217\n",
      "epoch: 10 batch_id: 67 Batch loss: 0.62899125\n",
      "epoch: 10 batch_id: 68 Batch loss: 0.35389864\n",
      "epoch: 10 batch_id: 69 Batch loss: 0.37196907\n",
      "epoch: 10 batch_id: 70 Batch loss: 0.5868262\n",
      "epoch: 10 batch_id: 71 Batch loss: 0.3996143\n",
      "epoch: 10 batch_id: 72 Batch loss: 0.59203005\n",
      "epoch: 10 batch_id: 73 Batch loss: 0.61107695\n",
      "epoch: 10 batch_id: 74 Batch loss: 0.38976857\n",
      "epoch: 10 batch_id: 75 Batch loss: 0.53221124\n",
      "epoch: 10 batch_id: 76 Batch loss: 0.4351035\n",
      "epoch: 10 batch_id: 77 Batch loss: 0.34911358\n",
      "epoch: 10 batch_id: 78 Batch loss: 0.5082536\n",
      "epoch: 10 batch_id: 79 Batch loss: 0.70238733\n",
      "epoch: 10 batch_id: 80 Batch loss: 0.3460482\n",
      "epoch: 10 batch_id: 81 Batch loss: 0.5731169\n",
      "epoch: 10 batch_id: 82 Batch loss: 0.35097602\n",
      "epoch: 10 batch_id: 83 Batch loss: 0.6448403\n",
      "epoch: 10 batch_id: 84 Batch loss: 0.44273102\n",
      "epoch: 10 batch_id: 85 Batch loss: 0.60355175\n",
      "epoch: 10 batch_id: 86 Batch loss: 0.67882997\n",
      "epoch: 10 batch_id: 87 Batch loss: 0.47636613\n",
      "epoch: 10 batch_id: 88 Batch loss: 0.39703014\n",
      "epoch: 10 batch_id: 89 Batch loss: 0.782877\n",
      "epoch: 10 batch_id: 90 Batch loss: 0.38084793\n",
      "epoch: 10 batch_id: 91 Batch loss: 0.5182314\n",
      "epoch: 10 batch_id: 92 Batch loss: 0.41197273\n",
      "epoch: 10 batch_id: 93 Batch loss: 0.39466545\n",
      "epoch: 10 batch_id: 94 Batch loss: 0.58378345\n",
      "epoch: 10 batch_id: 95 Batch loss: 0.84831774\n",
      "epoch: 10 batch_id: 96 Batch loss: 0.3901471\n",
      "epoch: 10 batch_id: 97 Batch loss: 0.71856046\n",
      "epoch: 10 batch_id: 98 Batch loss: 0.47357997\n",
      "epoch: 10 batch_id: 99 Batch loss: 0.5217461\n",
      "epoch: 10 batch_id: 100 Batch loss: 0.55650204\n",
      "epoch: 10 batch_id: 101 Batch loss: 0.4783796\n",
      "epoch: 10 batch_id: 102 Batch loss: 0.5334961\n",
      "epoch: 10 batch_id: 103 Batch loss: 0.5016585\n",
      "epoch: 10 batch_id: 104 Batch loss: 0.4532813\n",
      "epoch: 10 batch_id: 105 Batch loss: 0.6639085\n",
      "epoch: 10 batch_id: 106 Batch loss: 0.34138846\n",
      "epoch: 10 batch_id: 107 Batch loss: 0.56788737\n",
      "epoch: 10 batch_id: 108 Batch loss: 0.53145486\n",
      "epoch: 10 batch_id: 109 Batch loss: 0.42632255\n",
      "epoch: 10 batch_id: 110 Batch loss: 0.33080623\n",
      "epoch: 10 batch_id: 111 Batch loss: 0.62706745\n",
      "epoch: 10 batch_id: 112 Batch loss: 0.6431171\n",
      "epoch: 10 batch_id: 113 Batch loss: 0.32155198\n",
      "epoch: 10 batch_id: 114 Batch loss: 0.58052385\n",
      "epoch: 10 batch_id: 115 Batch loss: 0.39362958\n",
      "epoch: 10 batch_id: 116 Batch loss: 0.5254824\n",
      "epoch: 10 batch_id: 117 Batch loss: 0.58396834\n",
      "epoch: 10 batch_id: 118 Batch loss: 0.74268615\n",
      "epoch: 10 batch_id: 119 Batch loss: 0.67352533\n",
      "epoch: 10 batch_id: 120 Batch loss: 0.41068482\n",
      "epoch: 10 batch_id: 121 Batch loss: 0.56927454\n",
      "epoch: 10 batch_id: 122 Batch loss: 0.56028837\n",
      "epoch: 10 batch_id: 123 Batch loss: 0.51728284\n",
      "epoch: 10 batch_id: 124 Batch loss: 0.58782\n",
      "epoch: 10 batch_id: 125 Batch loss: 0.5258745\n",
      "epoch: 10 batch_id: 126 Batch loss: 0.5520712\n",
      "epoch: 10 batch_id: 127 Batch loss: 0.45108712\n",
      "epoch: 10 batch_id: 128 Batch loss: 0.28481922\n",
      "epoch: 10 batch_id: 129 Batch loss: 0.36147568\n",
      "epoch: 10 batch_id: 130 Batch loss: 0.42227715\n",
      "epoch: 10 batch_id: 131 Batch loss: 0.4728415\n",
      "epoch: 10 batch_id: 132 Batch loss: 0.47303554\n",
      "epoch: 10 batch_id: 133 Batch loss: 0.4464754\n",
      "epoch: 10 batch_id: 134 Batch loss: 0.49972042\n",
      "epoch: 10 batch_id: 135 Batch loss: 0.34012195\n",
      "epoch: 10 batch_id: 136 Batch loss: 0.593626\n",
      "epoch: 10 batch_id: 137 Batch loss: 0.67573774\n",
      "epoch: 10 batch_id: 138 Batch loss: 0.68101805\n",
      "epoch: 10 batch_id: 139 Batch loss: 0.4098571\n",
      "epoch: 10 batch_id: 140 Batch loss: 0.526098\n",
      "epoch: 10 batch_id: 141 Batch loss: 0.74202013\n",
      "epoch: 10 batch_id: 142 Batch loss: 0.6020553\n",
      "epoch: 10 batch_id: 143 Batch loss: 0.35967484\n",
      "epoch: 10 batch_id: 144 Batch loss: 0.5860235\n",
      "epoch: 10 batch_id: 145 Batch loss: 0.635529\n",
      "epoch: 10 batch_id: 146 Batch loss: 0.61596894\n",
      "epoch: 10 batch_id: 147 Batch loss: 0.46153843\n",
      "epoch: 10 batch_id: 148 Batch loss: 0.5786195\n",
      "epoch: 10 batch_id: 149 Batch loss: 0.6296207\n",
      "epoch: 10 batch_id: 150 Batch loss: 0.5524717\n",
      "epoch: 10 batch_id: 151 Batch loss: 0.5356058\n",
      "epoch: 10 batch_id: 152 Batch loss: 0.7594391\n",
      "epoch: 10 batch_id: 153 Batch loss: 0.34072396\n",
      "epoch: 10 batch_id: 154 Batch loss: 0.539711\n",
      "epoch: 10 batch_id: 155 Batch loss: 0.6528471\n",
      "epoch: 10 batch_id: 156 Batch loss: 0.43187004\n",
      "epoch: 10 batch_id: 157 Batch loss: 0.38419515\n",
      "epoch: 10 batch_id: 158 Batch loss: 0.51400685\n",
      "epoch: 10 batch_id: 159 Batch loss: 0.5481037\n",
      "epoch: 10 batch_id: 160 Batch loss: 0.53475934\n",
      "epoch: 10 batch_id: 161 Batch loss: 0.5913187\n",
      "epoch: 10 batch_id: 162 Batch loss: 0.6642143\n",
      "epoch: 10 batch_id: 163 Batch loss: 0.44784957\n",
      "epoch: 10 batch_id: 164 Batch loss: 0.58009195\n",
      "epoch: 10 batch_id: 165 Batch loss: 0.5492772\n",
      "epoch: 10 batch_id: 166 Batch loss: 0.5702883\n",
      "epoch: 10 batch_id: 167 Batch loss: 0.4845678\n",
      "epoch: 10 batch_id: 168 Batch loss: 0.33672944\n",
      "epoch: 10 batch_id: 169 Batch loss: 0.3435495\n",
      "epoch: 10 batch_id: 170 Batch loss: 0.5817595\n",
      "epoch: 10 batch_id: 171 Batch loss: 0.32387725\n",
      "epoch: 10 batch_id: 172 Batch loss: 0.53381413\n",
      "epoch: 10 batch_id: 173 Batch loss: 0.5686924\n",
      "epoch: 10 batch_id: 174 Batch loss: 0.43632278\n",
      "epoch: 10 batch_id: 175 Batch loss: 0.6102124\n",
      "epoch: 10 batch_id: 176 Batch loss: 0.57405114\n",
      "epoch: 10 batch_id: 177 Batch loss: 0.6505261\n",
      "epoch: 10 batch_id: 178 Batch loss: 0.409543\n",
      "epoch: 10 batch_id: 179 Batch loss: 0.41953465\n",
      "epoch: 10 batch_id: 180 Batch loss: 0.650586\n",
      "epoch: 10 batch_id: 181 Batch loss: 0.57806075\n",
      "epoch: 10 batch_id: 182 Batch loss: 0.51550084\n",
      "epoch: 10 batch_id: 183 Batch loss: 0.5650618\n",
      "epoch: 10 batch_id: 184 Batch loss: 0.7014143\n",
      "epoch: 10 batch_id: 185 Batch loss: 0.43273294\n",
      "epoch: 10 batch_id: 186 Batch loss: 0.7108354\n",
      "epoch: 10 batch_id: 187 Batch loss: 0.6564905\n",
      "epoch: 10 batch_id: 188 Batch loss: 0.5005716\n",
      "epoch: 10 batch_id: 189 Batch loss: 0.6561064\n",
      "epoch: 10 batch_id: 190 Batch loss: 0.581375\n",
      "epoch: 10 batch_id: 191 Batch loss: 0.47144613\n",
      "epoch: 10 batch_id: 192 Batch loss: 0.47351813\n",
      "epoch: 10 batch_id: 193 Batch loss: 0.625687\n",
      "epoch: 10 batch_id: 194 Batch loss: 0.43505096\n",
      "epoch: 10 batch_id: 195 Batch loss: 0.40052235\n",
      "epoch: 10 batch_id: 196 Batch loss: 0.59989375\n",
      "epoch: 10 batch_id: 197 Batch loss: 0.6354384\n",
      "epoch: 10 batch_id: 198 Batch loss: 0.60273224\n",
      "epoch: 10 batch_id: 199 Batch loss: 0.6116817\n",
      "epoch: 10 batch_id: 200 Batch loss: 0.64221156\n",
      "epoch: 10 batch_id: 201 Batch loss: 0.44952294\n",
      "epoch: 10 batch_id: 202 Batch loss: 0.5716535\n",
      "epoch: 10 batch_id: 203 Batch loss: 0.6250242\n",
      "epoch: 10 batch_id: 204 Batch loss: 0.43600994\n",
      "epoch: 10 batch_id: 205 Batch loss: 0.47761208\n",
      "epoch: 10 batch_id: 206 Batch loss: 0.561448\n",
      "epoch: 20 batch_id: 0 Batch loss: 0.58546907\n",
      "epoch: 20 batch_id: 1 Batch loss: 0.5032965\n",
      "epoch: 20 batch_id: 2 Batch loss: 0.56345856\n",
      "epoch: 20 batch_id: 3 Batch loss: 0.64730775\n",
      "epoch: 20 batch_id: 4 Batch loss: 0.52576953\n",
      "epoch: 20 batch_id: 5 Batch loss: 0.66110075\n",
      "epoch: 20 batch_id: 6 Batch loss: 0.40940377\n",
      "epoch: 20 batch_id: 7 Batch loss: 0.6010487\n",
      "epoch: 20 batch_id: 8 Batch loss: 0.54813117\n",
      "epoch: 20 batch_id: 9 Batch loss: 0.5821973\n",
      "epoch: 20 batch_id: 10 Batch loss: 0.3972615\n",
      "epoch: 20 batch_id: 11 Batch loss: 0.70675904\n",
      "epoch: 20 batch_id: 12 Batch loss: 0.58661795\n",
      "epoch: 20 batch_id: 13 Batch loss: 0.84957534\n",
      "epoch: 20 batch_id: 14 Batch loss: 0.60371244\n",
      "epoch: 20 batch_id: 15 Batch loss: 0.388808\n",
      "epoch: 20 batch_id: 16 Batch loss: 0.42858323\n",
      "epoch: 20 batch_id: 17 Batch loss: 0.57793176\n",
      "epoch: 20 batch_id: 18 Batch loss: 0.74087965\n",
      "epoch: 20 batch_id: 19 Batch loss: 0.58542895\n",
      "epoch: 20 batch_id: 20 Batch loss: 0.35998917\n",
      "epoch: 20 batch_id: 21 Batch loss: 0.42630684\n",
      "epoch: 20 batch_id: 22 Batch loss: 0.70761305\n",
      "epoch: 20 batch_id: 23 Batch loss: 0.4050428\n",
      "epoch: 20 batch_id: 24 Batch loss: 0.44054878\n",
      "epoch: 20 batch_id: 25 Batch loss: 0.6650363\n",
      "epoch: 20 batch_id: 26 Batch loss: 0.4988729\n",
      "epoch: 20 batch_id: 27 Batch loss: 0.6210022\n",
      "epoch: 20 batch_id: 28 Batch loss: 0.4832179\n",
      "epoch: 20 batch_id: 29 Batch loss: 0.4530446\n",
      "epoch: 20 batch_id: 30 Batch loss: 0.53457856\n",
      "epoch: 20 batch_id: 31 Batch loss: 0.74930817\n",
      "epoch: 20 batch_id: 32 Batch loss: 0.5703363\n",
      "epoch: 20 batch_id: 33 Batch loss: 0.40973252\n",
      "epoch: 20 batch_id: 34 Batch loss: 0.706711\n",
      "epoch: 20 batch_id: 35 Batch loss: 0.7062992\n",
      "epoch: 20 batch_id: 36 Batch loss: 0.8835312\n",
      "epoch: 20 batch_id: 37 Batch loss: 0.40394455\n",
      "epoch: 20 batch_id: 38 Batch loss: 0.56909007\n",
      "epoch: 20 batch_id: 39 Batch loss: 0.5260098\n",
      "epoch: 20 batch_id: 40 Batch loss: 0.36552393\n",
      "epoch: 20 batch_id: 41 Batch loss: 0.75351655\n",
      "epoch: 20 batch_id: 42 Batch loss: 0.5260159\n",
      "epoch: 20 batch_id: 43 Batch loss: 0.65423346\n",
      "epoch: 20 batch_id: 44 Batch loss: 0.30450904\n",
      "epoch: 20 batch_id: 45 Batch loss: 0.5575369\n",
      "epoch: 20 batch_id: 46 Batch loss: 0.4533915\n",
      "epoch: 20 batch_id: 47 Batch loss: 0.5065501\n",
      "epoch: 20 batch_id: 48 Batch loss: 0.5238672\n",
      "epoch: 20 batch_id: 49 Batch loss: 0.43662697\n",
      "epoch: 20 batch_id: 50 Batch loss: 0.45957425\n",
      "epoch: 20 batch_id: 51 Batch loss: 0.46398544\n",
      "epoch: 20 batch_id: 52 Batch loss: 0.79056734\n",
      "epoch: 20 batch_id: 53 Batch loss: 0.5797466\n",
      "epoch: 20 batch_id: 54 Batch loss: 0.7526216\n",
      "epoch: 20 batch_id: 55 Batch loss: 0.5519064\n",
      "epoch: 20 batch_id: 56 Batch loss: 0.56544596\n",
      "epoch: 20 batch_id: 57 Batch loss: 0.40573293\n",
      "epoch: 20 batch_id: 58 Batch loss: 0.6372075\n",
      "epoch: 20 batch_id: 59 Batch loss: 0.524825\n",
      "epoch: 20 batch_id: 60 Batch loss: 0.5592601\n",
      "epoch: 20 batch_id: 61 Batch loss: 0.42722228\n",
      "epoch: 20 batch_id: 62 Batch loss: 0.519683\n",
      "epoch: 20 batch_id: 63 Batch loss: 0.46922424\n",
      "epoch: 20 batch_id: 64 Batch loss: 0.57673323\n",
      "epoch: 20 batch_id: 65 Batch loss: 0.6852013\n",
      "epoch: 20 batch_id: 66 Batch loss: 0.33687958\n",
      "epoch: 20 batch_id: 67 Batch loss: 0.81290364\n",
      "epoch: 20 batch_id: 68 Batch loss: 0.55505604\n",
      "epoch: 20 batch_id: 69 Batch loss: 0.41612586\n",
      "epoch: 20 batch_id: 70 Batch loss: 0.6054253\n",
      "epoch: 20 batch_id: 71 Batch loss: 0.7790182\n",
      "epoch: 20 batch_id: 72 Batch loss: 0.33680212\n",
      "epoch: 20 batch_id: 73 Batch loss: 0.43851948\n",
      "epoch: 20 batch_id: 74 Batch loss: 0.3844918\n",
      "epoch: 20 batch_id: 75 Batch loss: 0.4247634\n",
      "epoch: 20 batch_id: 76 Batch loss: 0.5130274\n",
      "epoch: 20 batch_id: 77 Batch loss: 0.4773284\n",
      "epoch: 20 batch_id: 78 Batch loss: 0.7120752\n",
      "epoch: 20 batch_id: 79 Batch loss: 0.30093783\n",
      "epoch: 20 batch_id: 80 Batch loss: 0.6324965\n",
      "epoch: 20 batch_id: 81 Batch loss: 0.63082504\n",
      "epoch: 20 batch_id: 82 Batch loss: 0.52822334\n",
      "epoch: 20 batch_id: 83 Batch loss: 0.5640518\n",
      "epoch: 20 batch_id: 84 Batch loss: 0.5036201\n",
      "epoch: 20 batch_id: 85 Batch loss: 0.6396309\n",
      "epoch: 20 batch_id: 86 Batch loss: 0.51488096\n",
      "epoch: 20 batch_id: 87 Batch loss: 0.46699783\n",
      "epoch: 20 batch_id: 88 Batch loss: 0.3896333\n",
      "epoch: 20 batch_id: 89 Batch loss: 0.3839595\n",
      "epoch: 20 batch_id: 90 Batch loss: 0.7892326\n",
      "epoch: 20 batch_id: 91 Batch loss: 0.453526\n",
      "epoch: 20 batch_id: 92 Batch loss: 0.6983806\n",
      "epoch: 20 batch_id: 93 Batch loss: 0.38959485\n",
      "epoch: 20 batch_id: 94 Batch loss: 0.7059069\n",
      "epoch: 20 batch_id: 95 Batch loss: 0.27408865\n",
      "epoch: 20 batch_id: 96 Batch loss: 0.4751342\n",
      "epoch: 20 batch_id: 97 Batch loss: 0.5329151\n",
      "epoch: 20 batch_id: 98 Batch loss: 0.5031759\n",
      "epoch: 20 batch_id: 99 Batch loss: 0.567023\n",
      "epoch: 20 batch_id: 100 Batch loss: 0.39653346\n",
      "epoch: 20 batch_id: 101 Batch loss: 0.52486825\n",
      "epoch: 20 batch_id: 102 Batch loss: 0.45998973\n",
      "epoch: 20 batch_id: 103 Batch loss: 0.49963787\n",
      "epoch: 20 batch_id: 104 Batch loss: 0.5576396\n",
      "epoch: 20 batch_id: 105 Batch loss: 0.5087465\n",
      "epoch: 20 batch_id: 106 Batch loss: 0.40398285\n",
      "epoch: 20 batch_id: 107 Batch loss: 0.5719945\n",
      "epoch: 20 batch_id: 108 Batch loss: 0.74094194\n",
      "epoch: 20 batch_id: 109 Batch loss: 0.38519108\n",
      "epoch: 20 batch_id: 110 Batch loss: 0.5569484\n",
      "epoch: 20 batch_id: 111 Batch loss: 0.75759387\n",
      "epoch: 20 batch_id: 112 Batch loss: 0.577862\n",
      "epoch: 20 batch_id: 113 Batch loss: 0.50182045\n",
      "epoch: 20 batch_id: 114 Batch loss: 0.73261243\n",
      "epoch: 20 batch_id: 115 Batch loss: 0.44718194\n",
      "epoch: 20 batch_id: 116 Batch loss: 0.55910486\n",
      "epoch: 20 batch_id: 117 Batch loss: 1.1187301\n",
      "epoch: 20 batch_id: 118 Batch loss: 0.56618\n",
      "epoch: 20 batch_id: 119 Batch loss: 0.66819376\n",
      "epoch: 20 batch_id: 120 Batch loss: 0.67669344\n",
      "epoch: 20 batch_id: 121 Batch loss: 0.6406967\n",
      "epoch: 20 batch_id: 122 Batch loss: 0.44467378\n",
      "epoch: 20 batch_id: 123 Batch loss: 0.46840513\n",
      "epoch: 20 batch_id: 124 Batch loss: 0.51775146\n",
      "epoch: 20 batch_id: 125 Batch loss: 0.5165387\n",
      "epoch: 20 batch_id: 126 Batch loss: 0.47071815\n",
      "epoch: 20 batch_id: 127 Batch loss: 0.54195434\n",
      "epoch: 20 batch_id: 128 Batch loss: 0.4834642\n",
      "epoch: 20 batch_id: 129 Batch loss: 0.5593991\n",
      "epoch: 20 batch_id: 130 Batch loss: 0.5677473\n",
      "epoch: 20 batch_id: 131 Batch loss: 0.4328805\n",
      "epoch: 20 batch_id: 132 Batch loss: 0.768956\n",
      "epoch: 20 batch_id: 133 Batch loss: 0.4409849\n",
      "epoch: 20 batch_id: 134 Batch loss: 0.5967677\n",
      "epoch: 20 batch_id: 135 Batch loss: 0.50252944\n",
      "epoch: 20 batch_id: 136 Batch loss: 0.53229463\n",
      "epoch: 20 batch_id: 137 Batch loss: 0.5998921\n",
      "epoch: 20 batch_id: 138 Batch loss: 0.38016358\n",
      "epoch: 20 batch_id: 139 Batch loss: 0.43426856\n",
      "epoch: 20 batch_id: 140 Batch loss: 0.711973\n",
      "epoch: 20 batch_id: 141 Batch loss: 0.47609955\n",
      "epoch: 20 batch_id: 142 Batch loss: 0.5619021\n",
      "epoch: 20 batch_id: 143 Batch loss: 0.49217698\n",
      "epoch: 20 batch_id: 144 Batch loss: 0.5113477\n",
      "epoch: 20 batch_id: 145 Batch loss: 0.4873969\n",
      "epoch: 20 batch_id: 146 Batch loss: 0.39608613\n",
      "epoch: 20 batch_id: 147 Batch loss: 0.5915465\n",
      "epoch: 20 batch_id: 148 Batch loss: 0.5375644\n",
      "epoch: 20 batch_id: 149 Batch loss: 0.43189645\n",
      "epoch: 20 batch_id: 150 Batch loss: 0.5407032\n",
      "epoch: 20 batch_id: 151 Batch loss: 0.85504293\n",
      "epoch: 20 batch_id: 152 Batch loss: 0.40847337\n",
      "epoch: 20 batch_id: 153 Batch loss: 0.595245\n",
      "epoch: 20 batch_id: 154 Batch loss: 0.41363344\n",
      "epoch: 20 batch_id: 155 Batch loss: 0.6933232\n",
      "epoch: 20 batch_id: 156 Batch loss: 0.46902055\n",
      "epoch: 20 batch_id: 157 Batch loss: 0.38437447\n",
      "epoch: 20 batch_id: 158 Batch loss: 0.45328057\n",
      "epoch: 20 batch_id: 159 Batch loss: 0.56067324\n",
      "epoch: 20 batch_id: 160 Batch loss: 0.59508455\n",
      "epoch: 20 batch_id: 161 Batch loss: 0.5366423\n",
      "epoch: 20 batch_id: 162 Batch loss: 0.52082527\n",
      "epoch: 20 batch_id: 163 Batch loss: 0.5233457\n",
      "epoch: 20 batch_id: 164 Batch loss: 0.46888626\n",
      "epoch: 20 batch_id: 165 Batch loss: 0.55833286\n",
      "epoch: 20 batch_id: 166 Batch loss: 0.483424\n",
      "epoch: 20 batch_id: 167 Batch loss: 0.5431033\n",
      "epoch: 20 batch_id: 168 Batch loss: 0.44642448\n",
      "epoch: 20 batch_id: 169 Batch loss: 0.48017013\n",
      "epoch: 20 batch_id: 170 Batch loss: 0.43074563\n",
      "epoch: 20 batch_id: 171 Batch loss: 0.42681816\n",
      "epoch: 20 batch_id: 172 Batch loss: 0.55692184\n",
      "epoch: 20 batch_id: 173 Batch loss: 0.60550576\n",
      "epoch: 20 batch_id: 174 Batch loss: 0.6420524\n",
      "epoch: 20 batch_id: 175 Batch loss: 0.7006839\n",
      "epoch: 20 batch_id: 176 Batch loss: 0.33607343\n",
      "epoch: 20 batch_id: 177 Batch loss: 0.45908502\n",
      "epoch: 20 batch_id: 178 Batch loss: 0.37807196\n",
      "epoch: 20 batch_id: 179 Batch loss: 0.57039505\n",
      "epoch: 20 batch_id: 180 Batch loss: 0.44430274\n",
      "epoch: 20 batch_id: 181 Batch loss: 0.40613124\n",
      "epoch: 20 batch_id: 182 Batch loss: 0.67201614\n",
      "epoch: 20 batch_id: 183 Batch loss: 0.45968723\n",
      "epoch: 20 batch_id: 184 Batch loss: 0.95803857\n",
      "epoch: 20 batch_id: 185 Batch loss: 0.47335118\n",
      "epoch: 20 batch_id: 186 Batch loss: 0.5004566\n",
      "epoch: 20 batch_id: 187 Batch loss: 0.4521236\n",
      "epoch: 20 batch_id: 188 Batch loss: 0.5121064\n",
      "epoch: 20 batch_id: 189 Batch loss: 0.4114557\n",
      "epoch: 20 batch_id: 190 Batch loss: 0.52274346\n",
      "epoch: 20 batch_id: 191 Batch loss: 0.55994123\n",
      "epoch: 20 batch_id: 192 Batch loss: 0.39729008\n",
      "epoch: 20 batch_id: 193 Batch loss: 0.41966942\n",
      "epoch: 20 batch_id: 194 Batch loss: 0.6760759\n",
      "epoch: 20 batch_id: 195 Batch loss: 0.5396257\n",
      "epoch: 20 batch_id: 196 Batch loss: 0.52032137\n",
      "epoch: 20 batch_id: 197 Batch loss: 0.41557214\n",
      "epoch: 20 batch_id: 198 Batch loss: 0.5255531\n",
      "epoch: 20 batch_id: 199 Batch loss: 0.44622895\n",
      "epoch: 20 batch_id: 200 Batch loss: 0.6992512\n",
      "epoch: 20 batch_id: 201 Batch loss: 0.54899967\n",
      "epoch: 20 batch_id: 202 Batch loss: 0.700025\n",
      "epoch: 20 batch_id: 203 Batch loss: 0.47031862\n",
      "epoch: 20 batch_id: 204 Batch loss: 0.46516967\n",
      "epoch: 20 batch_id: 205 Batch loss: 0.5630955\n",
      "epoch: 20 batch_id: 206 Batch loss: 0.5520324\n",
      "epoch: 30 batch_id: 0 Batch loss: 0.43374833\n",
      "epoch: 30 batch_id: 1 Batch loss: 0.71949035\n",
      "epoch: 30 batch_id: 2 Batch loss: 0.40048248\n",
      "epoch: 30 batch_id: 3 Batch loss: 0.46638307\n",
      "epoch: 30 batch_id: 4 Batch loss: 0.49006817\n",
      "epoch: 30 batch_id: 5 Batch loss: 0.57236725\n",
      "epoch: 30 batch_id: 6 Batch loss: 0.54886836\n",
      "epoch: 30 batch_id: 7 Batch loss: 0.5131057\n",
      "epoch: 30 batch_id: 8 Batch loss: 0.56757283\n",
      "epoch: 30 batch_id: 9 Batch loss: 0.661502\n",
      "epoch: 30 batch_id: 10 Batch loss: 0.44540572\n",
      "epoch: 30 batch_id: 11 Batch loss: 0.775576\n",
      "epoch: 30 batch_id: 12 Batch loss: 0.4535912\n",
      "epoch: 30 batch_id: 13 Batch loss: 0.7485825\n",
      "epoch: 30 batch_id: 14 Batch loss: 0.5692098\n",
      "epoch: 30 batch_id: 15 Batch loss: 0.5669936\n",
      "epoch: 30 batch_id: 16 Batch loss: 0.5775443\n",
      "epoch: 30 batch_id: 17 Batch loss: 0.5939801\n",
      "epoch: 30 batch_id: 18 Batch loss: 0.73556566\n",
      "epoch: 30 batch_id: 19 Batch loss: 0.52719015\n",
      "epoch: 30 batch_id: 20 Batch loss: 0.5368761\n",
      "epoch: 30 batch_id: 21 Batch loss: 0.6681053\n",
      "epoch: 30 batch_id: 22 Batch loss: 0.5099278\n",
      "epoch: 30 batch_id: 23 Batch loss: 0.49550083\n",
      "epoch: 30 batch_id: 24 Batch loss: 0.54154956\n",
      "epoch: 30 batch_id: 25 Batch loss: 0.613871\n",
      "epoch: 30 batch_id: 26 Batch loss: 0.5604541\n",
      "epoch: 30 batch_id: 27 Batch loss: 0.51461756\n",
      "epoch: 30 batch_id: 28 Batch loss: 0.6203896\n",
      "epoch: 30 batch_id: 29 Batch loss: 0.41729194\n",
      "epoch: 30 batch_id: 30 Batch loss: 0.5256127\n",
      "epoch: 30 batch_id: 31 Batch loss: 0.44674256\n",
      "epoch: 30 batch_id: 32 Batch loss: 1.2361741\n",
      "epoch: 30 batch_id: 33 Batch loss: 0.3374613\n",
      "epoch: 30 batch_id: 34 Batch loss: 0.60614264\n",
      "epoch: 30 batch_id: 35 Batch loss: 0.50341624\n",
      "epoch: 30 batch_id: 36 Batch loss: 0.5144211\n",
      "epoch: 30 batch_id: 37 Batch loss: 0.45920658\n",
      "epoch: 30 batch_id: 38 Batch loss: 0.40595686\n",
      "epoch: 30 batch_id: 39 Batch loss: 0.37572122\n",
      "epoch: 30 batch_id: 40 Batch loss: 0.5657224\n",
      "epoch: 30 batch_id: 41 Batch loss: 0.51404214\n",
      "epoch: 30 batch_id: 42 Batch loss: 0.6422403\n",
      "epoch: 30 batch_id: 43 Batch loss: 0.43745652\n",
      "epoch: 30 batch_id: 44 Batch loss: 0.60832\n",
      "epoch: 30 batch_id: 45 Batch loss: 0.7140777\n",
      "epoch: 30 batch_id: 46 Batch loss: 0.5163545\n",
      "epoch: 30 batch_id: 47 Batch loss: 0.8271013\n",
      "epoch: 30 batch_id: 48 Batch loss: 0.5095932\n",
      "epoch: 30 batch_id: 49 Batch loss: 0.61637294\n",
      "epoch: 30 batch_id: 50 Batch loss: 79.369064\n",
      "epoch: 30 batch_id: 51 Batch loss: 0.460699\n",
      "epoch: 30 batch_id: 52 Batch loss: 0.52875054\n",
      "epoch: 30 batch_id: 53 Batch loss: 0.4539975\n",
      "epoch: 30 batch_id: 54 Batch loss: 0.8681698\n",
      "epoch: 30 batch_id: 55 Batch loss: 0.55545485\n",
      "epoch: 30 batch_id: 56 Batch loss: 0.41522762\n",
      "epoch: 30 batch_id: 57 Batch loss: 0.414349\n",
      "epoch: 30 batch_id: 58 Batch loss: 0.5872164\n",
      "epoch: 30 batch_id: 59 Batch loss: 0.7546364\n",
      "epoch: 30 batch_id: 60 Batch loss: 0.5626641\n",
      "epoch: 30 batch_id: 61 Batch loss: 0.68624663\n",
      "epoch: 30 batch_id: 62 Batch loss: 0.36354125\n",
      "epoch: 30 batch_id: 63 Batch loss: 0.58049697\n",
      "epoch: 30 batch_id: 64 Batch loss: 0.9333278\n",
      "epoch: 30 batch_id: 65 Batch loss: 0.5191613\n",
      "epoch: 30 batch_id: 66 Batch loss: 0.4235441\n",
      "epoch: 30 batch_id: 67 Batch loss: 0.39503163\n",
      "epoch: 30 batch_id: 68 Batch loss: 0.43320847\n",
      "epoch: 30 batch_id: 69 Batch loss: 0.5447027\n",
      "epoch: 30 batch_id: 70 Batch loss: 0.42769948\n",
      "epoch: 30 batch_id: 71 Batch loss: 0.47949073\n",
      "epoch: 30 batch_id: 72 Batch loss: 0.37952667\n",
      "epoch: 30 batch_id: 73 Batch loss: 0.542869\n",
      "epoch: 30 batch_id: 74 Batch loss: 0.5908375\n",
      "epoch: 30 batch_id: 75 Batch loss: 0.42992064\n",
      "epoch: 30 batch_id: 76 Batch loss: 0.61937\n",
      "epoch: 30 batch_id: 77 Batch loss: 0.5551893\n",
      "epoch: 30 batch_id: 78 Batch loss: 0.2927047\n",
      "epoch: 30 batch_id: 79 Batch loss: 0.41000852\n",
      "epoch: 30 batch_id: 80 Batch loss: 0.31526816\n",
      "epoch: 30 batch_id: 81 Batch loss: 0.43165267\n",
      "epoch: 30 batch_id: 82 Batch loss: 2.5389311\n",
      "epoch: 30 batch_id: 83 Batch loss: 0.46474373\n",
      "epoch: 30 batch_id: 84 Batch loss: 0.37802044\n",
      "epoch: 30 batch_id: 85 Batch loss: 0.63037056\n",
      "epoch: 30 batch_id: 86 Batch loss: 0.47677246\n",
      "epoch: 30 batch_id: 87 Batch loss: 0.49540442\n",
      "epoch: 30 batch_id: 88 Batch loss: 0.6028903\n",
      "epoch: 30 batch_id: 89 Batch loss: 0.5935288\n",
      "epoch: 30 batch_id: 90 Batch loss: 0.65265596\n",
      "epoch: 30 batch_id: 91 Batch loss: 0.5382691\n",
      "epoch: 30 batch_id: 92 Batch loss: 0.4383792\n",
      "epoch: 30 batch_id: 93 Batch loss: 0.4193916\n",
      "epoch: 30 batch_id: 94 Batch loss: 0.30259594\n",
      "epoch: 30 batch_id: 95 Batch loss: 0.47456008\n",
      "epoch: 30 batch_id: 96 Batch loss: 0.70379555\n",
      "epoch: 30 batch_id: 97 Batch loss: 0.6769374\n",
      "epoch: 30 batch_id: 98 Batch loss: 0.5676303\n",
      "epoch: 30 batch_id: 99 Batch loss: 0.5797127\n",
      "epoch: 30 batch_id: 100 Batch loss: 0.59304774\n",
      "epoch: 30 batch_id: 101 Batch loss: 0.6317003\n",
      "epoch: 30 batch_id: 102 Batch loss: 0.6028475\n",
      "epoch: 30 batch_id: 103 Batch loss: 0.36025766\n",
      "epoch: 30 batch_id: 104 Batch loss: 0.41756973\n",
      "epoch: 30 batch_id: 105 Batch loss: 0.49627888\n",
      "epoch: 30 batch_id: 106 Batch loss: 0.40922305\n",
      "epoch: 30 batch_id: 107 Batch loss: 0.6066379\n",
      "epoch: 30 batch_id: 108 Batch loss: 0.54880416\n",
      "epoch: 30 batch_id: 109 Batch loss: 0.64989805\n",
      "epoch: 30 batch_id: 110 Batch loss: 0.44359612\n",
      "epoch: 30 batch_id: 111 Batch loss: 0.4773632\n",
      "epoch: 30 batch_id: 112 Batch loss: 0.6562503\n",
      "epoch: 30 batch_id: 113 Batch loss: 0.46045715\n",
      "epoch: 30 batch_id: 114 Batch loss: 0.54352194\n",
      "epoch: 30 batch_id: 115 Batch loss: 0.4672786\n",
      "epoch: 30 batch_id: 116 Batch loss: 0.5991805\n",
      "epoch: 30 batch_id: 117 Batch loss: 0.42588416\n",
      "epoch: 30 batch_id: 118 Batch loss: 0.4351252\n",
      "epoch: 30 batch_id: 119 Batch loss: 0.43210876\n",
      "epoch: 30 batch_id: 120 Batch loss: 0.5914745\n",
      "epoch: 30 batch_id: 121 Batch loss: 0.6751276\n",
      "epoch: 30 batch_id: 122 Batch loss: 0.4463205\n",
      "epoch: 30 batch_id: 123 Batch loss: 0.44285122\n",
      "epoch: 30 batch_id: 124 Batch loss: 0.55232066\n",
      "epoch: 30 batch_id: 125 Batch loss: 0.52164954\n",
      "epoch: 30 batch_id: 126 Batch loss: 0.33661675\n",
      "epoch: 30 batch_id: 127 Batch loss: 0.45247364\n",
      "epoch: 30 batch_id: 128 Batch loss: 0.6703053\n",
      "epoch: 30 batch_id: 129 Batch loss: 0.6824755\n",
      "epoch: 30 batch_id: 130 Batch loss: 0.45963523\n",
      "epoch: 30 batch_id: 131 Batch loss: 0.4334856\n",
      "epoch: 30 batch_id: 132 Batch loss: 0.3316931\n",
      "epoch: 30 batch_id: 133 Batch loss: 0.5588053\n",
      "epoch: 30 batch_id: 134 Batch loss: 0.28407916\n",
      "epoch: 30 batch_id: 135 Batch loss: 0.41744813\n",
      "epoch: 30 batch_id: 136 Batch loss: 0.3241943\n",
      "epoch: 30 batch_id: 137 Batch loss: 0.6733883\n",
      "epoch: 30 batch_id: 138 Batch loss: 0.5011001\n",
      "epoch: 30 batch_id: 139 Batch loss: 0.3768605\n",
      "epoch: 30 batch_id: 140 Batch loss: 0.48803082\n",
      "epoch: 30 batch_id: 141 Batch loss: 0.44342637\n",
      "epoch: 30 batch_id: 142 Batch loss: 0.48965886\n",
      "epoch: 30 batch_id: 143 Batch loss: 0.3451144\n",
      "epoch: 30 batch_id: 144 Batch loss: 0.6097278\n",
      "epoch: 30 batch_id: 145 Batch loss: 0.46322402\n",
      "epoch: 30 batch_id: 146 Batch loss: 0.50998753\n",
      "epoch: 30 batch_id: 147 Batch loss: 0.43034813\n",
      "epoch: 30 batch_id: 148 Batch loss: 0.7809557\n",
      "epoch: 30 batch_id: 149 Batch loss: 0.37090066\n",
      "epoch: 30 batch_id: 150 Batch loss: 0.31510407\n",
      "epoch: 30 batch_id: 151 Batch loss: 0.34821174\n",
      "epoch: 30 batch_id: 152 Batch loss: 0.7175058\n",
      "epoch: 30 batch_id: 153 Batch loss: 0.62011707\n",
      "epoch: 30 batch_id: 154 Batch loss: 0.36377347\n",
      "epoch: 30 batch_id: 155 Batch loss: 0.5322366\n",
      "epoch: 30 batch_id: 156 Batch loss: 0.43522704\n",
      "epoch: 30 batch_id: 157 Batch loss: 0.4407447\n",
      "epoch: 30 batch_id: 158 Batch loss: 0.4531864\n",
      "epoch: 30 batch_id: 159 Batch loss: 0.5927249\n",
      "epoch: 30 batch_id: 160 Batch loss: 0.425616\n",
      "epoch: 30 batch_id: 161 Batch loss: 0.5033513\n",
      "epoch: 30 batch_id: 162 Batch loss: 0.5051269\n",
      "epoch: 30 batch_id: 163 Batch loss: 0.5576057\n",
      "epoch: 30 batch_id: 164 Batch loss: 0.538925\n",
      "epoch: 30 batch_id: 165 Batch loss: 0.5022856\n",
      "epoch: 30 batch_id: 166 Batch loss: 0.66278154\n",
      "epoch: 30 batch_id: 167 Batch loss: 0.40169156\n",
      "epoch: 30 batch_id: 168 Batch loss: 0.6133932\n",
      "epoch: 30 batch_id: 169 Batch loss: 0.7962134\n",
      "epoch: 30 batch_id: 170 Batch loss: 0.59947944\n",
      "epoch: 30 batch_id: 171 Batch loss: 0.60092527\n",
      "epoch: 30 batch_id: 172 Batch loss: 0.50696373\n",
      "epoch: 30 batch_id: 173 Batch loss: 0.47062987\n",
      "epoch: 30 batch_id: 174 Batch loss: 0.625068\n",
      "epoch: 30 batch_id: 175 Batch loss: 0.32354128\n",
      "epoch: 30 batch_id: 176 Batch loss: 0.57350975\n",
      "epoch: 30 batch_id: 177 Batch loss: 0.51096404\n",
      "epoch: 30 batch_id: 178 Batch loss: 0.6206863\n",
      "epoch: 30 batch_id: 179 Batch loss: 0.4914277\n",
      "epoch: 30 batch_id: 180 Batch loss: 0.5858643\n",
      "epoch: 30 batch_id: 181 Batch loss: 0.5762237\n",
      "epoch: 30 batch_id: 182 Batch loss: 0.5042532\n",
      "epoch: 30 batch_id: 183 Batch loss: 0.35117227\n",
      "epoch: 30 batch_id: 184 Batch loss: 0.4916504\n",
      "epoch: 30 batch_id: 185 Batch loss: 0.46561974\n",
      "epoch: 30 batch_id: 186 Batch loss: 0.5551105\n",
      "epoch: 30 batch_id: 187 Batch loss: 0.6127617\n",
      "epoch: 30 batch_id: 188 Batch loss: 0.37776756\n",
      "epoch: 30 batch_id: 189 Batch loss: 0.35470447\n",
      "epoch: 30 batch_id: 190 Batch loss: 0.6740711\n",
      "epoch: 30 batch_id: 191 Batch loss: 0.6372578\n",
      "epoch: 30 batch_id: 192 Batch loss: 0.55991095\n",
      "epoch: 30 batch_id: 193 Batch loss: 0.7084192\n",
      "epoch: 30 batch_id: 194 Batch loss: 0.6051993\n",
      "epoch: 30 batch_id: 195 Batch loss: 0.528924\n",
      "epoch: 30 batch_id: 196 Batch loss: 0.735609\n",
      "epoch: 30 batch_id: 197 Batch loss: 0.3592122\n",
      "epoch: 30 batch_id: 198 Batch loss: 0.41732845\n",
      "epoch: 30 batch_id: 199 Batch loss: 0.5679684\n",
      "epoch: 30 batch_id: 200 Batch loss: 0.5027334\n",
      "epoch: 30 batch_id: 201 Batch loss: 0.5869178\n",
      "epoch: 30 batch_id: 202 Batch loss: 0.34635392\n",
      "epoch: 30 batch_id: 203 Batch loss: 0.2963971\n",
      "epoch: 30 batch_id: 204 Batch loss: 0.46379977\n",
      "epoch: 30 batch_id: 205 Batch loss: 0.48263663\n",
      "epoch: 30 batch_id: 206 Batch loss: 0.5906026\n",
      "epoch: 40 batch_id: 0 Batch loss: 0.540822\n",
      "epoch: 40 batch_id: 1 Batch loss: 0.39819527\n",
      "epoch: 40 batch_id: 2 Batch loss: 0.64936316\n",
      "epoch: 40 batch_id: 3 Batch loss: 0.5740334\n",
      "epoch: 40 batch_id: 4 Batch loss: 0.5684762\n",
      "epoch: 40 batch_id: 5 Batch loss: 0.4221402\n",
      "epoch: 40 batch_id: 6 Batch loss: 0.5676356\n",
      "epoch: 40 batch_id: 7 Batch loss: 0.9757428\n",
      "epoch: 40 batch_id: 8 Batch loss: 0.30698904\n",
      "epoch: 40 batch_id: 9 Batch loss: 0.44988194\n",
      "epoch: 40 batch_id: 10 Batch loss: 0.4697602\n",
      "epoch: 40 batch_id: 11 Batch loss: 0.6608296\n",
      "epoch: 40 batch_id: 12 Batch loss: 0.52468145\n",
      "epoch: 40 batch_id: 13 Batch loss: 0.49272633\n",
      "epoch: 40 batch_id: 14 Batch loss: 0.6700405\n",
      "epoch: 40 batch_id: 15 Batch loss: 0.42218903\n",
      "epoch: 40 batch_id: 16 Batch loss: 0.6361878\n",
      "epoch: 40 batch_id: 17 Batch loss: 0.52567774\n",
      "epoch: 40 batch_id: 18 Batch loss: 0.5097769\n",
      "epoch: 40 batch_id: 19 Batch loss: 0.812043\n",
      "epoch: 40 batch_id: 20 Batch loss: 0.3322009\n",
      "epoch: 40 batch_id: 21 Batch loss: 0.6485444\n",
      "epoch: 40 batch_id: 22 Batch loss: 0.36991227\n",
      "epoch: 40 batch_id: 23 Batch loss: 0.46298528\n",
      "epoch: 40 batch_id: 24 Batch loss: 0.5367286\n",
      "epoch: 40 batch_id: 25 Batch loss: 0.59654254\n",
      "epoch: 40 batch_id: 26 Batch loss: 0.49228233\n",
      "epoch: 40 batch_id: 27 Batch loss: 0.77120036\n",
      "epoch: 40 batch_id: 28 Batch loss: 0.67158705\n",
      "epoch: 40 batch_id: 29 Batch loss: 0.46344584\n",
      "epoch: 40 batch_id: 30 Batch loss: 0.5394792\n",
      "epoch: 40 batch_id: 31 Batch loss: 0.45428193\n",
      "epoch: 40 batch_id: 32 Batch loss: 0.3089076\n",
      "epoch: 40 batch_id: 33 Batch loss: 0.546722\n",
      "epoch: 40 batch_id: 34 Batch loss: 0.48324248\n",
      "epoch: 40 batch_id: 35 Batch loss: 0.6386534\n",
      "epoch: 40 batch_id: 36 Batch loss: 0.59605455\n",
      "epoch: 40 batch_id: 37 Batch loss: 0.61168844\n",
      "epoch: 40 batch_id: 38 Batch loss: 0.5993608\n",
      "epoch: 40 batch_id: 39 Batch loss: 0.39501488\n",
      "epoch: 40 batch_id: 40 Batch loss: 0.4368399\n",
      "epoch: 40 batch_id: 41 Batch loss: 0.5944781\n",
      "epoch: 40 batch_id: 42 Batch loss: 0.9938156\n",
      "epoch: 40 batch_id: 43 Batch loss: 0.34546608\n",
      "epoch: 40 batch_id: 44 Batch loss: 0.48557496\n",
      "epoch: 40 batch_id: 45 Batch loss: 0.45267296\n",
      "epoch: 40 batch_id: 46 Batch loss: 0.7899578\n",
      "epoch: 40 batch_id: 47 Batch loss: 0.6994728\n",
      "epoch: 40 batch_id: 48 Batch loss: 0.6296605\n",
      "epoch: 40 batch_id: 49 Batch loss: 0.6433555\n",
      "epoch: 40 batch_id: 50 Batch loss: 0.57386506\n",
      "epoch: 40 batch_id: 51 Batch loss: 0.35551366\n",
      "epoch: 40 batch_id: 52 Batch loss: 0.46509543\n",
      "epoch: 40 batch_id: 53 Batch loss: 0.42496228\n",
      "epoch: 40 batch_id: 54 Batch loss: 0.29025856\n",
      "epoch: 40 batch_id: 55 Batch loss: 0.6336593\n",
      "epoch: 40 batch_id: 56 Batch loss: 0.44660568\n",
      "epoch: 40 batch_id: 57 Batch loss: 0.64826834\n",
      "epoch: 40 batch_id: 58 Batch loss: 0.58530116\n",
      "epoch: 40 batch_id: 59 Batch loss: 0.2912352\n",
      "epoch: 40 batch_id: 60 Batch loss: 0.40098822\n",
      "epoch: 40 batch_id: 61 Batch loss: 0.5102137\n",
      "epoch: 40 batch_id: 62 Batch loss: 0.70873064\n",
      "epoch: 40 batch_id: 63 Batch loss: 0.44058305\n",
      "epoch: 40 batch_id: 64 Batch loss: 0.52392685\n",
      "epoch: 40 batch_id: 65 Batch loss: 0.39125276\n",
      "epoch: 40 batch_id: 66 Batch loss: 0.6541931\n",
      "epoch: 40 batch_id: 67 Batch loss: 0.44377425\n",
      "epoch: 40 batch_id: 68 Batch loss: 0.49563\n",
      "epoch: 40 batch_id: 69 Batch loss: 0.63985044\n",
      "epoch: 40 batch_id: 70 Batch loss: 0.5688781\n",
      "epoch: 40 batch_id: 71 Batch loss: 0.48310426\n",
      "epoch: 40 batch_id: 72 Batch loss: 0.49694306\n",
      "epoch: 40 batch_id: 73 Batch loss: 0.31071126\n",
      "epoch: 40 batch_id: 74 Batch loss: 0.6910045\n",
      "epoch: 40 batch_id: 75 Batch loss: 0.395865\n",
      "epoch: 40 batch_id: 76 Batch loss: 0.6224358\n",
      "epoch: 40 batch_id: 77 Batch loss: 0.556214\n",
      "epoch: 40 batch_id: 78 Batch loss: 0.4061905\n",
      "epoch: 40 batch_id: 79 Batch loss: 0.40102562\n",
      "epoch: 40 batch_id: 80 Batch loss: 0.37891385\n",
      "epoch: 40 batch_id: 81 Batch loss: 0.30469272\n",
      "epoch: 40 batch_id: 82 Batch loss: 0.44953305\n",
      "epoch: 40 batch_id: 83 Batch loss: 0.6315646\n",
      "epoch: 40 batch_id: 84 Batch loss: 0.5627193\n",
      "epoch: 40 batch_id: 85 Batch loss: 0.5536705\n",
      "epoch: 40 batch_id: 86 Batch loss: 0.42041633\n",
      "epoch: 40 batch_id: 87 Batch loss: 0.46144295\n",
      "epoch: 40 batch_id: 88 Batch loss: 0.60009104\n",
      "epoch: 40 batch_id: 89 Batch loss: 0.6794222\n",
      "epoch: 40 batch_id: 90 Batch loss: 0.43998238\n",
      "epoch: 40 batch_id: 91 Batch loss: 0.29817116\n",
      "epoch: 40 batch_id: 92 Batch loss: 0.49242592\n",
      "epoch: 40 batch_id: 93 Batch loss: 0.6232663\n",
      "epoch: 40 batch_id: 94 Batch loss: 0.4341555\n",
      "epoch: 40 batch_id: 95 Batch loss: 0.52444315\n",
      "epoch: 40 batch_id: 96 Batch loss: 0.47290787\n",
      "epoch: 40 batch_id: 97 Batch loss: 0.755101\n",
      "epoch: 40 batch_id: 98 Batch loss: 0.6166435\n",
      "epoch: 40 batch_id: 99 Batch loss: 0.52626854\n",
      "epoch: 40 batch_id: 100 Batch loss: 0.5418565\n",
      "epoch: 40 batch_id: 101 Batch loss: 0.24446881\n",
      "epoch: 40 batch_id: 102 Batch loss: 0.6558998\n",
      "epoch: 40 batch_id: 103 Batch loss: 0.7538631\n",
      "epoch: 40 batch_id: 104 Batch loss: 0.32545686\n",
      "epoch: 40 batch_id: 105 Batch loss: 0.46435428\n",
      "epoch: 40 batch_id: 106 Batch loss: 0.660601\n",
      "epoch: 40 batch_id: 107 Batch loss: 0.6403616\n",
      "epoch: 40 batch_id: 108 Batch loss: 0.4417118\n",
      "epoch: 40 batch_id: 109 Batch loss: 0.5894071\n",
      "epoch: 40 batch_id: 110 Batch loss: 0.45942876\n",
      "epoch: 40 batch_id: 111 Batch loss: 0.46446645\n",
      "epoch: 40 batch_id: 112 Batch loss: 0.44734654\n",
      "epoch: 40 batch_id: 113 Batch loss: 0.51663494\n",
      "epoch: 40 batch_id: 114 Batch loss: 0.5190907\n",
      "epoch: 40 batch_id: 115 Batch loss: 0.53073156\n",
      "epoch: 40 batch_id: 116 Batch loss: 0.46217892\n",
      "epoch: 40 batch_id: 117 Batch loss: 0.7359066\n",
      "epoch: 40 batch_id: 118 Batch loss: 0.48180932\n",
      "epoch: 40 batch_id: 119 Batch loss: 0.5866578\n",
      "epoch: 40 batch_id: 120 Batch loss: 0.40959287\n",
      "epoch: 40 batch_id: 121 Batch loss: 0.47109023\n",
      "epoch: 40 batch_id: 122 Batch loss: 0.55551726\n",
      "epoch: 40 batch_id: 123 Batch loss: 0.5550032\n",
      "epoch: 40 batch_id: 124 Batch loss: 0.42495042\n",
      "epoch: 40 batch_id: 125 Batch loss: 0.40524888\n",
      "epoch: 40 batch_id: 126 Batch loss: 0.48661017\n",
      "epoch: 40 batch_id: 127 Batch loss: 0.59358215\n",
      "epoch: 40 batch_id: 128 Batch loss: 0.46486932\n",
      "epoch: 40 batch_id: 129 Batch loss: 0.56738967\n",
      "epoch: 40 batch_id: 130 Batch loss: 0.5688873\n",
      "epoch: 40 batch_id: 131 Batch loss: 0.5792702\n",
      "epoch: 40 batch_id: 132 Batch loss: 0.5315611\n",
      "epoch: 40 batch_id: 133 Batch loss: 0.531249\n",
      "epoch: 40 batch_id: 134 Batch loss: 0.4206227\n",
      "epoch: 40 batch_id: 135 Batch loss: 0.49944192\n",
      "epoch: 40 batch_id: 136 Batch loss: 0.5229059\n",
      "epoch: 40 batch_id: 137 Batch loss: 0.58101827\n",
      "epoch: 40 batch_id: 138 Batch loss: 0.63001347\n",
      "epoch: 40 batch_id: 139 Batch loss: 0.6900142\n",
      "epoch: 40 batch_id: 140 Batch loss: 0.48074982\n",
      "epoch: 40 batch_id: 141 Batch loss: 0.60283166\n",
      "epoch: 40 batch_id: 142 Batch loss: 0.54813755\n",
      "epoch: 40 batch_id: 143 Batch loss: 0.516626\n",
      "epoch: 40 batch_id: 144 Batch loss: 0.36560807\n",
      "epoch: 40 batch_id: 145 Batch loss: 0.5136266\n",
      "epoch: 40 batch_id: 146 Batch loss: 0.574105\n",
      "epoch: 40 batch_id: 147 Batch loss: 0.36232814\n",
      "epoch: 40 batch_id: 148 Batch loss: 0.52845263\n",
      "epoch: 40 batch_id: 149 Batch loss: 0.550062\n",
      "epoch: 40 batch_id: 150 Batch loss: 0.6111904\n",
      "epoch: 40 batch_id: 151 Batch loss: 0.47427005\n",
      "epoch: 40 batch_id: 152 Batch loss: 0.43684867\n",
      "epoch: 40 batch_id: 153 Batch loss: 0.64184844\n",
      "epoch: 40 batch_id: 154 Batch loss: 0.37449548\n",
      "epoch: 40 batch_id: 155 Batch loss: 0.4729251\n",
      "epoch: 40 batch_id: 156 Batch loss: 4.2735376\n",
      "epoch: 40 batch_id: 157 Batch loss: 0.5682891\n",
      "epoch: 40 batch_id: 158 Batch loss: 0.481319\n",
      "epoch: 40 batch_id: 159 Batch loss: 0.52768064\n",
      "epoch: 40 batch_id: 160 Batch loss: 0.5650427\n",
      "epoch: 40 batch_id: 161 Batch loss: 0.41630512\n",
      "epoch: 40 batch_id: 162 Batch loss: 0.60237783\n",
      "epoch: 40 batch_id: 163 Batch loss: 0.75155985\n",
      "epoch: 40 batch_id: 164 Batch loss: 0.5133943\n",
      "epoch: 40 batch_id: 165 Batch loss: 0.65143204\n",
      "epoch: 40 batch_id: 166 Batch loss: 0.59600836\n",
      "epoch: 40 batch_id: 167 Batch loss: 0.671405\n",
      "epoch: 40 batch_id: 168 Batch loss: 0.4903063\n",
      "epoch: 40 batch_id: 169 Batch loss: 0.4256133\n",
      "epoch: 40 batch_id: 170 Batch loss: 0.45055678\n",
      "epoch: 40 batch_id: 171 Batch loss: 0.38302696\n",
      "epoch: 40 batch_id: 172 Batch loss: 0.47374198\n",
      "epoch: 40 batch_id: 173 Batch loss: 0.4596682\n",
      "epoch: 40 batch_id: 174 Batch loss: 0.51553434\n",
      "epoch: 40 batch_id: 175 Batch loss: 0.4098008\n",
      "epoch: 40 batch_id: 176 Batch loss: 0.5875583\n",
      "epoch: 40 batch_id: 177 Batch loss: 0.5089956\n",
      "epoch: 40 batch_id: 178 Batch loss: 0.66971135\n",
      "epoch: 40 batch_id: 179 Batch loss: 0.47812003\n",
      "epoch: 40 batch_id: 180 Batch loss: 0.40024212\n",
      "epoch: 40 batch_id: 181 Batch loss: 0.48706517\n",
      "epoch: 40 batch_id: 182 Batch loss: 0.39702687\n",
      "epoch: 40 batch_id: 183 Batch loss: 0.4570495\n",
      "epoch: 40 batch_id: 184 Batch loss: 0.4823136\n",
      "epoch: 40 batch_id: 185 Batch loss: 0.65149283\n",
      "epoch: 40 batch_id: 186 Batch loss: 0.36265433\n",
      "epoch: 40 batch_id: 187 Batch loss: 0.47462884\n",
      "epoch: 40 batch_id: 188 Batch loss: 0.3654167\n",
      "epoch: 40 batch_id: 189 Batch loss: 0.56436896\n",
      "epoch: 40 batch_id: 190 Batch loss: 0.6268991\n",
      "epoch: 40 batch_id: 191 Batch loss: 0.64386296\n",
      "epoch: 40 batch_id: 192 Batch loss: 0.441179\n",
      "epoch: 40 batch_id: 193 Batch loss: 0.5166084\n",
      "epoch: 40 batch_id: 194 Batch loss: 0.4624508\n",
      "epoch: 40 batch_id: 195 Batch loss: 0.34869888\n",
      "epoch: 40 batch_id: 196 Batch loss: 0.6164647\n",
      "epoch: 40 batch_id: 197 Batch loss: 0.41914925\n",
      "epoch: 40 batch_id: 198 Batch loss: 0.5125307\n",
      "epoch: 40 batch_id: 199 Batch loss: 0.88008326\n",
      "epoch: 40 batch_id: 200 Batch loss: 0.5299016\n",
      "epoch: 40 batch_id: 201 Batch loss: 0.7008448\n",
      "epoch: 40 batch_id: 202 Batch loss: 0.3539814\n",
      "epoch: 40 batch_id: 203 Batch loss: 0.5251106\n",
      "epoch: 40 batch_id: 204 Batch loss: 0.4819682\n",
      "epoch: 40 batch_id: 205 Batch loss: 0.49542296\n",
      "epoch: 40 batch_id: 206 Batch loss: 0.34487292\n",
      "epoch: 50 batch_id: 0 Batch loss: 0.3286958\n",
      "epoch: 50 batch_id: 1 Batch loss: 0.41199416\n",
      "epoch: 50 batch_id: 2 Batch loss: 0.30693114\n",
      "epoch: 50 batch_id: 3 Batch loss: 0.7550809\n",
      "epoch: 50 batch_id: 4 Batch loss: 0.6530954\n",
      "epoch: 50 batch_id: 5 Batch loss: 0.67310315\n",
      "epoch: 50 batch_id: 6 Batch loss: 0.5568847\n",
      "epoch: 50 batch_id: 7 Batch loss: 0.47557423\n",
      "epoch: 50 batch_id: 8 Batch loss: 0.4725503\n",
      "epoch: 50 batch_id: 9 Batch loss: 0.4954809\n",
      "epoch: 50 batch_id: 10 Batch loss: 0.60974276\n",
      "epoch: 50 batch_id: 11 Batch loss: 0.55713576\n",
      "epoch: 50 batch_id: 12 Batch loss: 0.6844893\n",
      "epoch: 50 batch_id: 13 Batch loss: 0.4572299\n",
      "epoch: 50 batch_id: 14 Batch loss: 0.41293922\n",
      "epoch: 50 batch_id: 15 Batch loss: 0.45668212\n",
      "epoch: 50 batch_id: 16 Batch loss: 0.4266194\n",
      "epoch: 50 batch_id: 17 Batch loss: 0.38392976\n",
      "epoch: 50 batch_id: 18 Batch loss: 0.38301423\n",
      "epoch: 50 batch_id: 19 Batch loss: 0.60169196\n",
      "epoch: 50 batch_id: 20 Batch loss: 0.3811908\n",
      "epoch: 50 batch_id: 21 Batch loss: 0.46083814\n",
      "epoch: 50 batch_id: 22 Batch loss: 0.3896681\n",
      "epoch: 50 batch_id: 23 Batch loss: 0.39876214\n",
      "epoch: 50 batch_id: 24 Batch loss: 0.8314979\n",
      "epoch: 50 batch_id: 25 Batch loss: 0.4465005\n",
      "epoch: 50 batch_id: 26 Batch loss: 0.57071286\n",
      "epoch: 50 batch_id: 27 Batch loss: 0.60162354\n",
      "epoch: 50 batch_id: 28 Batch loss: 0.3957296\n",
      "epoch: 50 batch_id: 29 Batch loss: 0.48505196\n",
      "epoch: 50 batch_id: 30 Batch loss: 0.78720033\n",
      "epoch: 50 batch_id: 31 Batch loss: 0.50606287\n",
      "epoch: 50 batch_id: 32 Batch loss: 0.61244476\n",
      "epoch: 50 batch_id: 33 Batch loss: 0.4954252\n",
      "epoch: 50 batch_id: 34 Batch loss: 0.39910802\n",
      "epoch: 50 batch_id: 35 Batch loss: 0.40499154\n",
      "epoch: 50 batch_id: 36 Batch loss: 0.50447565\n",
      "epoch: 50 batch_id: 37 Batch loss: 0.5346615\n",
      "epoch: 50 batch_id: 38 Batch loss: 0.46056235\n",
      "epoch: 50 batch_id: 39 Batch loss: 0.5881043\n",
      "epoch: 50 batch_id: 40 Batch loss: 0.4545465\n",
      "epoch: 50 batch_id: 41 Batch loss: 0.449655\n",
      "epoch: 50 batch_id: 42 Batch loss: 0.5694694\n",
      "epoch: 50 batch_id: 43 Batch loss: 0.61348224\n",
      "epoch: 50 batch_id: 44 Batch loss: 0.5899385\n",
      "epoch: 50 batch_id: 45 Batch loss: 0.42438096\n",
      "epoch: 50 batch_id: 46 Batch loss: 0.476435\n",
      "epoch: 50 batch_id: 47 Batch loss: 0.55065376\n",
      "epoch: 50 batch_id: 48 Batch loss: 0.45174617\n",
      "epoch: 50 batch_id: 49 Batch loss: 0.46270326\n",
      "epoch: 50 batch_id: 50 Batch loss: 0.6724393\n",
      "epoch: 50 batch_id: 51 Batch loss: 0.47950292\n",
      "epoch: 50 batch_id: 52 Batch loss: 0.6409027\n",
      "epoch: 50 batch_id: 53 Batch loss: 0.44069725\n",
      "epoch: 50 batch_id: 54 Batch loss: 0.48363456\n",
      "epoch: 50 batch_id: 55 Batch loss: 0.5519688\n",
      "epoch: 50 batch_id: 56 Batch loss: 0.5254904\n",
      "epoch: 50 batch_id: 57 Batch loss: 0.41892582\n",
      "epoch: 50 batch_id: 58 Batch loss: 0.59951526\n",
      "epoch: 50 batch_id: 59 Batch loss: 0.39174247\n",
      "epoch: 50 batch_id: 60 Batch loss: 0.40148467\n",
      "epoch: 50 batch_id: 61 Batch loss: 0.5862939\n",
      "epoch: 50 batch_id: 62 Batch loss: 0.6569928\n",
      "epoch: 50 batch_id: 63 Batch loss: 0.5374697\n",
      "epoch: 50 batch_id: 64 Batch loss: 0.6606794\n",
      "epoch: 50 batch_id: 65 Batch loss: 0.47136214\n",
      "epoch: 50 batch_id: 66 Batch loss: 0.60734916\n",
      "epoch: 50 batch_id: 67 Batch loss: 0.4846705\n",
      "epoch: 50 batch_id: 68 Batch loss: 0.47691873\n",
      "epoch: 50 batch_id: 69 Batch loss: 0.7240794\n",
      "epoch: 50 batch_id: 70 Batch loss: 0.43728152\n",
      "epoch: 50 batch_id: 71 Batch loss: 0.56026465\n",
      "epoch: 50 batch_id: 72 Batch loss: 0.604121\n",
      "epoch: 50 batch_id: 73 Batch loss: 0.35853708\n",
      "epoch: 50 batch_id: 74 Batch loss: 0.61526686\n",
      "epoch: 50 batch_id: 75 Batch loss: 0.5879107\n",
      "epoch: 50 batch_id: 76 Batch loss: 0.38693273\n",
      "epoch: 50 batch_id: 77 Batch loss: 0.5373857\n",
      "epoch: 50 batch_id: 78 Batch loss: 0.54238\n",
      "epoch: 50 batch_id: 79 Batch loss: 0.53431964\n",
      "epoch: 50 batch_id: 80 Batch loss: 0.77328265\n",
      "epoch: 50 batch_id: 81 Batch loss: 0.61049277\n",
      "epoch: 50 batch_id: 82 Batch loss: 0.52040774\n",
      "epoch: 50 batch_id: 83 Batch loss: 0.6255895\n",
      "epoch: 50 batch_id: 84 Batch loss: 0.4816155\n",
      "epoch: 50 batch_id: 85 Batch loss: 0.74487185\n",
      "epoch: 50 batch_id: 86 Batch loss: 0.57388574\n",
      "epoch: 50 batch_id: 87 Batch loss: 0.5509708\n",
      "epoch: 50 batch_id: 88 Batch loss: 0.42061478\n",
      "epoch: 50 batch_id: 89 Batch loss: 0.4643337\n",
      "epoch: 50 batch_id: 90 Batch loss: 0.5463556\n",
      "epoch: 50 batch_id: 91 Batch loss: 0.4035375\n",
      "epoch: 50 batch_id: 92 Batch loss: 0.6022149\n",
      "epoch: 50 batch_id: 93 Batch loss: 0.55231965\n",
      "epoch: 50 batch_id: 94 Batch loss: 0.48660117\n",
      "epoch: 50 batch_id: 95 Batch loss: 0.3822987\n",
      "epoch: 50 batch_id: 96 Batch loss: 0.5681678\n",
      "epoch: 50 batch_id: 97 Batch loss: 0.49187467\n",
      "epoch: 50 batch_id: 98 Batch loss: 0.6669434\n",
      "epoch: 50 batch_id: 99 Batch loss: 0.67118883\n",
      "epoch: 50 batch_id: 100 Batch loss: 0.71534616\n",
      "epoch: 50 batch_id: 101 Batch loss: 0.5532297\n",
      "epoch: 50 batch_id: 102 Batch loss: 0.68157077\n",
      "epoch: 50 batch_id: 103 Batch loss: 0.6024308\n",
      "epoch: 50 batch_id: 104 Batch loss: 0.50767213\n",
      "epoch: 50 batch_id: 105 Batch loss: 0.6120338\n",
      "epoch: 50 batch_id: 106 Batch loss: 0.3860912\n",
      "epoch: 50 batch_id: 107 Batch loss: 0.43092623\n",
      "epoch: 50 batch_id: 108 Batch loss: 0.631623\n",
      "epoch: 50 batch_id: 109 Batch loss: 0.5237352\n",
      "epoch: 50 batch_id: 110 Batch loss: 0.39244324\n",
      "epoch: 50 batch_id: 111 Batch loss: 0.3764272\n",
      "epoch: 50 batch_id: 112 Batch loss: 0.49750686\n",
      "epoch: 50 batch_id: 113 Batch loss: 0.427284\n",
      "epoch: 50 batch_id: 114 Batch loss: 0.5276144\n",
      "epoch: 50 batch_id: 115 Batch loss: 0.48393685\n",
      "epoch: 50 batch_id: 116 Batch loss: 0.7668346\n",
      "epoch: 50 batch_id: 117 Batch loss: 0.46269295\n",
      "epoch: 50 batch_id: 118 Batch loss: 0.3580892\n",
      "epoch: 50 batch_id: 119 Batch loss: 0.4024067\n",
      "epoch: 50 batch_id: 120 Batch loss: 0.55036306\n",
      "epoch: 50 batch_id: 121 Batch loss: 0.5478647\n",
      "epoch: 50 batch_id: 122 Batch loss: 0.6480176\n",
      "epoch: 50 batch_id: 123 Batch loss: 0.4206796\n",
      "epoch: 50 batch_id: 124 Batch loss: 0.6731444\n",
      "epoch: 50 batch_id: 125 Batch loss: 0.4337262\n",
      "epoch: 50 batch_id: 126 Batch loss: 0.546777\n",
      "epoch: 50 batch_id: 127 Batch loss: 0.64699525\n",
      "epoch: 50 batch_id: 128 Batch loss: 0.34151366\n",
      "epoch: 50 batch_id: 129 Batch loss: 0.35367644\n",
      "epoch: 50 batch_id: 130 Batch loss: 0.65953386\n",
      "epoch: 50 batch_id: 131 Batch loss: 0.53085726\n",
      "epoch: 50 batch_id: 132 Batch loss: 0.62338907\n",
      "epoch: 50 batch_id: 133 Batch loss: 0.47835347\n",
      "epoch: 50 batch_id: 134 Batch loss: 0.71555114\n",
      "epoch: 50 batch_id: 135 Batch loss: 0.55659705\n",
      "epoch: 50 batch_id: 136 Batch loss: 0.5162316\n",
      "epoch: 50 batch_id: 137 Batch loss: 0.6101303\n",
      "epoch: 50 batch_id: 138 Batch loss: 0.5353834\n",
      "epoch: 50 batch_id: 139 Batch loss: 0.4162615\n",
      "epoch: 50 batch_id: 140 Batch loss: 0.5189661\n",
      "epoch: 50 batch_id: 141 Batch loss: 0.31216273\n",
      "epoch: 50 batch_id: 142 Batch loss: 0.5664465\n",
      "epoch: 50 batch_id: 143 Batch loss: 0.46406233\n",
      "epoch: 50 batch_id: 144 Batch loss: 0.44852296\n",
      "epoch: 50 batch_id: 145 Batch loss: 0.45652816\n",
      "epoch: 50 batch_id: 146 Batch loss: 0.6205133\n",
      "epoch: 50 batch_id: 147 Batch loss: 0.46201393\n",
      "epoch: 50 batch_id: 148 Batch loss: 0.45478085\n",
      "epoch: 50 batch_id: 149 Batch loss: 0.37016463\n",
      "epoch: 50 batch_id: 150 Batch loss: 0.6349748\n",
      "epoch: 50 batch_id: 151 Batch loss: 0.6323773\n",
      "epoch: 50 batch_id: 152 Batch loss: 0.67424524\n",
      "epoch: 50 batch_id: 153 Batch loss: 0.37271038\n",
      "epoch: 50 batch_id: 154 Batch loss: 0.48490316\n",
      "epoch: 50 batch_id: 155 Batch loss: 0.643334\n",
      "epoch: 50 batch_id: 156 Batch loss: 0.6680047\n",
      "epoch: 50 batch_id: 157 Batch loss: 0.5076968\n",
      "epoch: 50 batch_id: 158 Batch loss: 0.36733302\n",
      "epoch: 50 batch_id: 159 Batch loss: 0.44470206\n",
      "epoch: 50 batch_id: 160 Batch loss: 0.45671925\n",
      "epoch: 50 batch_id: 161 Batch loss: 0.62706727\n",
      "epoch: 50 batch_id: 162 Batch loss: 0.50833\n",
      "epoch: 50 batch_id: 163 Batch loss: 0.66726553\n",
      "epoch: 50 batch_id: 164 Batch loss: 0.67553514\n",
      "epoch: 50 batch_id: 165 Batch loss: 0.4462999\n",
      "epoch: 50 batch_id: 166 Batch loss: 0.6927477\n",
      "epoch: 50 batch_id: 167 Batch loss: 0.583238\n",
      "epoch: 50 batch_id: 168 Batch loss: 0.58367515\n",
      "epoch: 50 batch_id: 169 Batch loss: 0.41721234\n",
      "epoch: 50 batch_id: 170 Batch loss: 0.3525074\n",
      "epoch: 50 batch_id: 171 Batch loss: 0.38628343\n",
      "epoch: 50 batch_id: 172 Batch loss: 0.5528845\n",
      "epoch: 50 batch_id: 173 Batch loss: 0.41579694\n",
      "epoch: 50 batch_id: 174 Batch loss: 0.62102616\n",
      "epoch: 50 batch_id: 175 Batch loss: 0.6395725\n",
      "epoch: 50 batch_id: 176 Batch loss: 0.42850533\n",
      "epoch: 50 batch_id: 177 Batch loss: 0.5543744\n",
      "epoch: 50 batch_id: 178 Batch loss: 0.47527897\n",
      "epoch: 50 batch_id: 179 Batch loss: 0.5003133\n",
      "epoch: 50 batch_id: 180 Batch loss: 0.43266705\n",
      "epoch: 50 batch_id: 181 Batch loss: 0.6215635\n",
      "epoch: 50 batch_id: 182 Batch loss: 0.5305225\n",
      "epoch: 50 batch_id: 183 Batch loss: 0.46246684\n",
      "epoch: 50 batch_id: 184 Batch loss: 0.43218508\n",
      "epoch: 50 batch_id: 185 Batch loss: 0.49571627\n",
      "epoch: 50 batch_id: 186 Batch loss: 0.39525127\n",
      "epoch: 50 batch_id: 187 Batch loss: 0.75094986\n",
      "epoch: 50 batch_id: 188 Batch loss: 0.6154522\n",
      "epoch: 50 batch_id: 189 Batch loss: 0.49840924\n",
      "epoch: 50 batch_id: 190 Batch loss: 0.46422055\n",
      "epoch: 50 batch_id: 191 Batch loss: 0.45898324\n",
      "epoch: 50 batch_id: 192 Batch loss: 0.55042315\n",
      "epoch: 50 batch_id: 193 Batch loss: 0.5694179\n",
      "epoch: 50 batch_id: 194 Batch loss: 0.48248306\n",
      "epoch: 50 batch_id: 195 Batch loss: 0.45842135\n",
      "epoch: 50 batch_id: 196 Batch loss: 0.4499659\n",
      "epoch: 50 batch_id: 197 Batch loss: 0.44985047\n",
      "epoch: 50 batch_id: 198 Batch loss: 0.8436907\n",
      "epoch: 50 batch_id: 199 Batch loss: 0.42065033\n",
      "epoch: 50 batch_id: 200 Batch loss: 0.4292984\n",
      "epoch: 50 batch_id: 201 Batch loss: 0.6113607\n",
      "epoch: 50 batch_id: 202 Batch loss: 0.57372016\n",
      "epoch: 50 batch_id: 203 Batch loss: 0.52162683\n",
      "epoch: 50 batch_id: 204 Batch loss: 0.54860365\n",
      "epoch: 50 batch_id: 205 Batch loss: 0.4666954\n",
      "epoch: 50 batch_id: 206 Batch loss: 0.44054422\n",
      "epoch: 60 batch_id: 0 Batch loss: 0.430412\n",
      "epoch: 60 batch_id: 1 Batch loss: 0.45762935\n",
      "epoch: 60 batch_id: 2 Batch loss: 0.36814317\n",
      "epoch: 60 batch_id: 3 Batch loss: 0.5078966\n",
      "epoch: 60 batch_id: 4 Batch loss: 0.5682468\n",
      "epoch: 60 batch_id: 5 Batch loss: 0.29871365\n",
      "epoch: 60 batch_id: 6 Batch loss: 0.4909171\n",
      "epoch: 60 batch_id: 7 Batch loss: 0.57982534\n",
      "epoch: 60 batch_id: 8 Batch loss: 0.71412283\n",
      "epoch: 60 batch_id: 9 Batch loss: 0.41727567\n",
      "epoch: 60 batch_id: 10 Batch loss: 0.34669316\n",
      "epoch: 60 batch_id: 11 Batch loss: 0.5447544\n",
      "epoch: 60 batch_id: 12 Batch loss: 0.48244822\n",
      "epoch: 60 batch_id: 13 Batch loss: 0.5852915\n",
      "epoch: 60 batch_id: 14 Batch loss: 0.4405201\n",
      "epoch: 60 batch_id: 15 Batch loss: 0.5595388\n",
      "epoch: 60 batch_id: 16 Batch loss: 0.45730302\n",
      "epoch: 60 batch_id: 17 Batch loss: 0.50883067\n",
      "epoch: 60 batch_id: 18 Batch loss: 0.62390155\n",
      "epoch: 60 batch_id: 19 Batch loss: 0.5368732\n",
      "epoch: 60 batch_id: 20 Batch loss: 0.5190125\n",
      "epoch: 60 batch_id: 21 Batch loss: 0.57638144\n",
      "epoch: 60 batch_id: 22 Batch loss: 0.58775246\n",
      "epoch: 60 batch_id: 23 Batch loss: 0.32790145\n",
      "epoch: 60 batch_id: 24 Batch loss: 0.32295924\n",
      "epoch: 60 batch_id: 25 Batch loss: 0.5079679\n",
      "epoch: 60 batch_id: 26 Batch loss: 0.5685274\n",
      "epoch: 60 batch_id: 27 Batch loss: 0.74342966\n",
      "epoch: 60 batch_id: 28 Batch loss: 0.396986\n",
      "epoch: 60 batch_id: 29 Batch loss: 0.51538545\n",
      "epoch: 60 batch_id: 30 Batch loss: 0.38223296\n",
      "epoch: 60 batch_id: 31 Batch loss: 0.5100209\n",
      "epoch: 60 batch_id: 32 Batch loss: 0.40583336\n",
      "epoch: 60 batch_id: 33 Batch loss: 0.53773206\n",
      "epoch: 60 batch_id: 34 Batch loss: 0.39116883\n",
      "epoch: 60 batch_id: 35 Batch loss: 0.6632141\n",
      "epoch: 60 batch_id: 36 Batch loss: 0.6453394\n",
      "epoch: 60 batch_id: 37 Batch loss: 0.5973728\n",
      "epoch: 60 batch_id: 38 Batch loss: 0.44278747\n",
      "epoch: 60 batch_id: 39 Batch loss: 0.6702361\n",
      "epoch: 60 batch_id: 40 Batch loss: 0.4196076\n",
      "epoch: 60 batch_id: 41 Batch loss: 0.7300072\n",
      "epoch: 60 batch_id: 42 Batch loss: 0.48738304\n",
      "epoch: 60 batch_id: 43 Batch loss: 0.5880095\n",
      "epoch: 60 batch_id: 44 Batch loss: 0.40185338\n",
      "epoch: 60 batch_id: 45 Batch loss: 0.5446894\n",
      "epoch: 60 batch_id: 46 Batch loss: 0.52784413\n",
      "epoch: 60 batch_id: 47 Batch loss: 0.38521287\n",
      "epoch: 60 batch_id: 48 Batch loss: 0.55414724\n",
      "epoch: 60 batch_id: 49 Batch loss: 0.42894495\n",
      "epoch: 60 batch_id: 50 Batch loss: 0.53304553\n",
      "epoch: 60 batch_id: 51 Batch loss: 0.48972693\n",
      "epoch: 60 batch_id: 52 Batch loss: 0.6684126\n",
      "epoch: 60 batch_id: 53 Batch loss: 0.42583328\n",
      "epoch: 60 batch_id: 54 Batch loss: 0.5486245\n",
      "epoch: 60 batch_id: 55 Batch loss: 0.4659645\n",
      "epoch: 60 batch_id: 56 Batch loss: 0.47961885\n",
      "epoch: 60 batch_id: 57 Batch loss: 0.520706\n",
      "epoch: 60 batch_id: 58 Batch loss: 0.6494426\n",
      "epoch: 60 batch_id: 59 Batch loss: 0.38154808\n",
      "epoch: 60 batch_id: 60 Batch loss: 0.57450366\n",
      "epoch: 60 batch_id: 61 Batch loss: 0.37420875\n",
      "epoch: 60 batch_id: 62 Batch loss: 0.66423035\n",
      "epoch: 60 batch_id: 63 Batch loss: 0.5178719\n",
      "epoch: 60 batch_id: 64 Batch loss: 0.5309524\n",
      "epoch: 60 batch_id: 65 Batch loss: 0.43492958\n",
      "epoch: 60 batch_id: 66 Batch loss: 0.62078935\n",
      "epoch: 60 batch_id: 67 Batch loss: 0.43364915\n",
      "epoch: 60 batch_id: 68 Batch loss: 0.44046557\n",
      "epoch: 60 batch_id: 69 Batch loss: 0.32156983\n",
      "epoch: 60 batch_id: 70 Batch loss: 0.47398636\n",
      "epoch: 60 batch_id: 71 Batch loss: 0.48106286\n",
      "epoch: 60 batch_id: 72 Batch loss: 0.5721419\n",
      "epoch: 60 batch_id: 73 Batch loss: 0.73071593\n",
      "epoch: 60 batch_id: 74 Batch loss: 0.51033205\n",
      "epoch: 60 batch_id: 75 Batch loss: 0.352275\n",
      "epoch: 60 batch_id: 76 Batch loss: 0.5736822\n",
      "epoch: 60 batch_id: 77 Batch loss: 0.56375796\n",
      "epoch: 60 batch_id: 78 Batch loss: 0.5403492\n",
      "epoch: 60 batch_id: 79 Batch loss: 0.49431157\n",
      "epoch: 60 batch_id: 80 Batch loss: 0.6318366\n",
      "epoch: 60 batch_id: 81 Batch loss: 0.36924994\n",
      "epoch: 60 batch_id: 82 Batch loss: 0.5695639\n",
      "epoch: 60 batch_id: 83 Batch loss: 0.48487175\n",
      "epoch: 60 batch_id: 84 Batch loss: 0.60356855\n",
      "epoch: 60 batch_id: 85 Batch loss: 0.48893586\n",
      "epoch: 60 batch_id: 86 Batch loss: 0.6172903\n",
      "epoch: 60 batch_id: 87 Batch loss: 0.51536614\n",
      "epoch: 60 batch_id: 88 Batch loss: 0.5629626\n",
      "epoch: 60 batch_id: 89 Batch loss: 0.6811191\n",
      "epoch: 60 batch_id: 90 Batch loss: 0.47955775\n",
      "epoch: 60 batch_id: 91 Batch loss: 0.38193616\n",
      "epoch: 60 batch_id: 92 Batch loss: 0.47775972\n",
      "epoch: 60 batch_id: 93 Batch loss: 0.5841391\n",
      "epoch: 60 batch_id: 94 Batch loss: 0.36965284\n",
      "epoch: 60 batch_id: 95 Batch loss: 0.5013799\n",
      "epoch: 60 batch_id: 96 Batch loss: 0.6773834\n",
      "epoch: 60 batch_id: 97 Batch loss: 0.5429933\n",
      "epoch: 60 batch_id: 98 Batch loss: 0.6515242\n",
      "epoch: 60 batch_id: 99 Batch loss: 0.28628835\n",
      "epoch: 60 batch_id: 100 Batch loss: 0.48551685\n",
      "epoch: 60 batch_id: 101 Batch loss: 0.6456952\n",
      "epoch: 60 batch_id: 102 Batch loss: 0.5699228\n",
      "epoch: 60 batch_id: 103 Batch loss: 0.59382904\n",
      "epoch: 60 batch_id: 104 Batch loss: 0.43614206\n",
      "epoch: 60 batch_id: 105 Batch loss: 0.567346\n",
      "epoch: 60 batch_id: 106 Batch loss: 0.585238\n",
      "epoch: 60 batch_id: 107 Batch loss: 0.6618278\n",
      "epoch: 60 batch_id: 108 Batch loss: 0.50136214\n",
      "epoch: 60 batch_id: 109 Batch loss: 0.48374695\n",
      "epoch: 60 batch_id: 110 Batch loss: 0.43250054\n",
      "epoch: 60 batch_id: 111 Batch loss: 0.32235116\n",
      "epoch: 60 batch_id: 112 Batch loss: 0.7554379\n",
      "epoch: 60 batch_id: 113 Batch loss: 0.28803122\n",
      "epoch: 60 batch_id: 114 Batch loss: 0.6913322\n",
      "epoch: 60 batch_id: 115 Batch loss: 0.83300394\n",
      "epoch: 60 batch_id: 116 Batch loss: 0.5298719\n",
      "epoch: 60 batch_id: 117 Batch loss: 0.37452343\n",
      "epoch: 60 batch_id: 118 Batch loss: 0.41835558\n",
      "epoch: 60 batch_id: 119 Batch loss: 0.41103736\n",
      "epoch: 60 batch_id: 120 Batch loss: 0.3114708\n",
      "epoch: 60 batch_id: 121 Batch loss: 0.44835892\n",
      "epoch: 60 batch_id: 122 Batch loss: 0.32456616\n",
      "epoch: 60 batch_id: 123 Batch loss: 0.65247124\n",
      "epoch: 60 batch_id: 124 Batch loss: 0.34503776\n",
      "epoch: 60 batch_id: 125 Batch loss: 0.42438087\n",
      "epoch: 60 batch_id: 126 Batch loss: 0.467753\n",
      "epoch: 60 batch_id: 127 Batch loss: 0.49177605\n",
      "epoch: 60 batch_id: 128 Batch loss: 0.41240647\n",
      "epoch: 60 batch_id: 129 Batch loss: 0.37450463\n",
      "epoch: 60 batch_id: 130 Batch loss: 0.48893613\n",
      "epoch: 60 batch_id: 131 Batch loss: 0.45689034\n",
      "epoch: 60 batch_id: 132 Batch loss: 0.5611832\n",
      "epoch: 60 batch_id: 133 Batch loss: 0.5330014\n",
      "epoch: 60 batch_id: 134 Batch loss: 0.65773845\n",
      "epoch: 60 batch_id: 135 Batch loss: 0.40302387\n",
      "epoch: 60 batch_id: 136 Batch loss: 0.93953073\n",
      "epoch: 60 batch_id: 137 Batch loss: 0.5608562\n",
      "epoch: 60 batch_id: 138 Batch loss: 0.5326773\n",
      "epoch: 60 batch_id: 139 Batch loss: 0.43161988\n",
      "epoch: 60 batch_id: 140 Batch loss: 0.63550097\n",
      "epoch: 60 batch_id: 141 Batch loss: 0.5776983\n",
      "epoch: 60 batch_id: 142 Batch loss: 0.3586647\n",
      "epoch: 60 batch_id: 143 Batch loss: 0.6748567\n",
      "epoch: 60 batch_id: 144 Batch loss: 0.48887187\n",
      "epoch: 60 batch_id: 145 Batch loss: 0.64045113\n",
      "epoch: 60 batch_id: 146 Batch loss: 0.32850915\n",
      "epoch: 60 batch_id: 147 Batch loss: 0.5016596\n",
      "epoch: 60 batch_id: 148 Batch loss: 0.39707243\n",
      "epoch: 60 batch_id: 149 Batch loss: 0.45514145\n",
      "epoch: 60 batch_id: 150 Batch loss: 0.59455985\n",
      "epoch: 60 batch_id: 151 Batch loss: 0.40852058\n",
      "epoch: 60 batch_id: 152 Batch loss: 0.5417469\n",
      "epoch: 60 batch_id: 153 Batch loss: 0.71133757\n",
      "epoch: 60 batch_id: 154 Batch loss: 0.39292222\n",
      "epoch: 60 batch_id: 155 Batch loss: 0.46536088\n",
      "epoch: 60 batch_id: 156 Batch loss: 0.4808503\n",
      "epoch: 60 batch_id: 157 Batch loss: 0.39689755\n",
      "epoch: 60 batch_id: 158 Batch loss: 0.5514075\n",
      "epoch: 60 batch_id: 159 Batch loss: 0.50223005\n",
      "epoch: 60 batch_id: 160 Batch loss: 0.4244357\n",
      "epoch: 60 batch_id: 161 Batch loss: 0.5136202\n",
      "epoch: 60 batch_id: 162 Batch loss: 0.41618118\n",
      "epoch: 60 batch_id: 163 Batch loss: 0.41202393\n",
      "epoch: 60 batch_id: 164 Batch loss: 0.8784067\n",
      "epoch: 60 batch_id: 165 Batch loss: 0.4735762\n",
      "epoch: 60 batch_id: 166 Batch loss: 0.64527696\n",
      "epoch: 60 batch_id: 167 Batch loss: 0.54794616\n",
      "epoch: 60 batch_id: 168 Batch loss: 0.7750119\n",
      "epoch: 60 batch_id: 169 Batch loss: 0.51199937\n",
      "epoch: 60 batch_id: 170 Batch loss: 0.50721365\n",
      "epoch: 60 batch_id: 171 Batch loss: 0.3408545\n",
      "epoch: 60 batch_id: 172 Batch loss: 0.6032661\n",
      "epoch: 60 batch_id: 173 Batch loss: 0.57607836\n",
      "epoch: 60 batch_id: 174 Batch loss: 0.49924874\n",
      "epoch: 60 batch_id: 175 Batch loss: 0.36481872\n",
      "epoch: 60 batch_id: 176 Batch loss: 0.61373365\n",
      "epoch: 60 batch_id: 177 Batch loss: 0.7411351\n",
      "epoch: 60 batch_id: 178 Batch loss: 0.40097705\n",
      "epoch: 60 batch_id: 179 Batch loss: 0.6660609\n",
      "epoch: 60 batch_id: 180 Batch loss: 0.37842667\n",
      "epoch: 60 batch_id: 181 Batch loss: 0.60735655\n",
      "epoch: 60 batch_id: 182 Batch loss: 0.5629968\n",
      "epoch: 60 batch_id: 183 Batch loss: 0.5385476\n",
      "epoch: 60 batch_id: 184 Batch loss: 0.6379923\n",
      "epoch: 60 batch_id: 185 Batch loss: 0.584827\n",
      "epoch: 60 batch_id: 186 Batch loss: 0.5258653\n",
      "epoch: 60 batch_id: 187 Batch loss: 0.6177891\n",
      "epoch: 60 batch_id: 188 Batch loss: 0.57499456\n",
      "epoch: 60 batch_id: 189 Batch loss: 0.5474276\n",
      "epoch: 60 batch_id: 190 Batch loss: 0.7022654\n",
      "epoch: 60 batch_id: 191 Batch loss: 0.44285953\n",
      "epoch: 60 batch_id: 192 Batch loss: 0.5001439\n",
      "epoch: 60 batch_id: 193 Batch loss: 0.38139778\n",
      "epoch: 60 batch_id: 194 Batch loss: 0.52318376\n",
      "epoch: 60 batch_id: 195 Batch loss: 0.5596648\n",
      "epoch: 60 batch_id: 196 Batch loss: 0.526896\n",
      "epoch: 60 batch_id: 197 Batch loss: 0.5299792\n",
      "epoch: 60 batch_id: 198 Batch loss: 0.8980722\n",
      "epoch: 60 batch_id: 199 Batch loss: 0.5160451\n",
      "epoch: 60 batch_id: 200 Batch loss: 0.41132098\n",
      "epoch: 60 batch_id: 201 Batch loss: 0.3753533\n",
      "epoch: 60 batch_id: 202 Batch loss: 0.38930848\n",
      "epoch: 60 batch_id: 203 Batch loss: 0.5161553\n",
      "epoch: 60 batch_id: 204 Batch loss: 0.49394006\n",
      "epoch: 60 batch_id: 205 Batch loss: 0.5610597\n",
      "epoch: 60 batch_id: 206 Batch loss: 0.54812247\n",
      "epoch: 70 batch_id: 0 Batch loss: 0.42269334\n",
      "epoch: 70 batch_id: 1 Batch loss: 0.79199874\n",
      "epoch: 70 batch_id: 2 Batch loss: 0.39785737\n",
      "epoch: 70 batch_id: 3 Batch loss: 0.5310532\n",
      "epoch: 70 batch_id: 4 Batch loss: 0.36422265\n",
      "epoch: 70 batch_id: 5 Batch loss: 0.37152573\n",
      "epoch: 70 batch_id: 6 Batch loss: 0.5054849\n",
      "epoch: 70 batch_id: 7 Batch loss: 0.39018166\n",
      "epoch: 70 batch_id: 8 Batch loss: 0.62955964\n",
      "epoch: 70 batch_id: 9 Batch loss: 0.6877115\n",
      "epoch: 70 batch_id: 10 Batch loss: 0.39008594\n",
      "epoch: 70 batch_id: 11 Batch loss: 0.6104023\n",
      "epoch: 70 batch_id: 12 Batch loss: 0.62762284\n",
      "epoch: 70 batch_id: 13 Batch loss: 0.4244243\n",
      "epoch: 70 batch_id: 14 Batch loss: 0.4224903\n",
      "epoch: 70 batch_id: 15 Batch loss: 0.44496262\n",
      "epoch: 70 batch_id: 16 Batch loss: 0.5702177\n",
      "epoch: 70 batch_id: 17 Batch loss: 0.40876794\n",
      "epoch: 70 batch_id: 18 Batch loss: 0.684646\n",
      "epoch: 70 batch_id: 19 Batch loss: 0.5063845\n",
      "epoch: 70 batch_id: 20 Batch loss: 0.2661937\n",
      "epoch: 70 batch_id: 21 Batch loss: 0.42134833\n",
      "epoch: 70 batch_id: 22 Batch loss: 0.61154306\n",
      "epoch: 70 batch_id: 23 Batch loss: 0.4655636\n",
      "epoch: 70 batch_id: 24 Batch loss: 0.5619092\n",
      "epoch: 70 batch_id: 25 Batch loss: 0.60025823\n",
      "epoch: 70 batch_id: 26 Batch loss: 0.71074927\n",
      "epoch: 70 batch_id: 27 Batch loss: 0.48013404\n",
      "epoch: 70 batch_id: 28 Batch loss: 0.4079038\n",
      "epoch: 70 batch_id: 29 Batch loss: 0.5011218\n",
      "epoch: 70 batch_id: 30 Batch loss: 0.61367875\n",
      "epoch: 70 batch_id: 31 Batch loss: 0.4567833\n",
      "epoch: 70 batch_id: 32 Batch loss: 0.67892015\n",
      "epoch: 70 batch_id: 33 Batch loss: 0.5799436\n",
      "epoch: 70 batch_id: 34 Batch loss: 0.5999198\n",
      "epoch: 70 batch_id: 35 Batch loss: 0.6392221\n",
      "epoch: 70 batch_id: 36 Batch loss: 0.6241858\n",
      "epoch: 70 batch_id: 37 Batch loss: 0.47754222\n",
      "epoch: 70 batch_id: 38 Batch loss: 0.48045707\n",
      "epoch: 70 batch_id: 39 Batch loss: 0.4168512\n",
      "epoch: 70 batch_id: 40 Batch loss: 0.72156054\n",
      "epoch: 70 batch_id: 41 Batch loss: 0.4881155\n",
      "epoch: 70 batch_id: 42 Batch loss: 0.33349273\n",
      "epoch: 70 batch_id: 43 Batch loss: 0.38997528\n",
      "epoch: 70 batch_id: 44 Batch loss: 0.4103781\n",
      "epoch: 70 batch_id: 45 Batch loss: 0.4850151\n",
      "epoch: 70 batch_id: 46 Batch loss: 0.5608874\n",
      "epoch: 70 batch_id: 47 Batch loss: 0.5754804\n",
      "epoch: 70 batch_id: 48 Batch loss: 0.42485213\n",
      "epoch: 70 batch_id: 49 Batch loss: 0.6061426\n",
      "epoch: 70 batch_id: 50 Batch loss: 0.6314846\n",
      "epoch: 70 batch_id: 51 Batch loss: 0.56357634\n",
      "epoch: 70 batch_id: 52 Batch loss: 0.4101593\n",
      "epoch: 70 batch_id: 53 Batch loss: 0.5115699\n",
      "epoch: 70 batch_id: 54 Batch loss: 0.4018304\n",
      "epoch: 70 batch_id: 55 Batch loss: 0.5444352\n",
      "epoch: 70 batch_id: 56 Batch loss: 0.56916255\n",
      "epoch: 70 batch_id: 57 Batch loss: 0.3389717\n",
      "epoch: 70 batch_id: 58 Batch loss: 0.45800886\n",
      "epoch: 70 batch_id: 59 Batch loss: 0.5123886\n",
      "epoch: 70 batch_id: 60 Batch loss: 0.49711514\n",
      "epoch: 70 batch_id: 61 Batch loss: 0.43036053\n",
      "epoch: 70 batch_id: 62 Batch loss: 0.40062386\n",
      "epoch: 70 batch_id: 63 Batch loss: 0.41496807\n",
      "epoch: 70 batch_id: 64 Batch loss: 0.4372784\n",
      "epoch: 70 batch_id: 65 Batch loss: 0.51100516\n",
      "epoch: 70 batch_id: 66 Batch loss: 0.5301248\n",
      "epoch: 70 batch_id: 67 Batch loss: 0.47131497\n",
      "epoch: 70 batch_id: 68 Batch loss: 0.5626679\n",
      "epoch: 70 batch_id: 69 Batch loss: 0.48742244\n",
      "epoch: 70 batch_id: 70 Batch loss: 0.7574139\n",
      "epoch: 70 batch_id: 71 Batch loss: 0.5056676\n",
      "epoch: 70 batch_id: 72 Batch loss: 0.44247672\n",
      "epoch: 70 batch_id: 73 Batch loss: 0.46215156\n",
      "epoch: 70 batch_id: 74 Batch loss: 0.33733514\n",
      "epoch: 70 batch_id: 75 Batch loss: 0.55188334\n",
      "epoch: 70 batch_id: 76 Batch loss: 0.46048808\n",
      "epoch: 70 batch_id: 77 Batch loss: 0.57126874\n",
      "epoch: 70 batch_id: 78 Batch loss: 0.5393693\n",
      "epoch: 70 batch_id: 79 Batch loss: 0.573955\n",
      "epoch: 70 batch_id: 80 Batch loss: 0.6024625\n",
      "epoch: 70 batch_id: 81 Batch loss: 0.5417072\n",
      "epoch: 70 batch_id: 82 Batch loss: 0.43556693\n",
      "epoch: 70 batch_id: 83 Batch loss: 0.55801284\n",
      "epoch: 70 batch_id: 84 Batch loss: 0.41985685\n",
      "epoch: 70 batch_id: 85 Batch loss: 0.55891013\n",
      "epoch: 70 batch_id: 86 Batch loss: 0.54224324\n",
      "epoch: 70 batch_id: 87 Batch loss: 0.44352654\n",
      "epoch: 70 batch_id: 88 Batch loss: 0.35653603\n",
      "epoch: 70 batch_id: 89 Batch loss: 0.48667243\n",
      "epoch: 70 batch_id: 90 Batch loss: 0.5680177\n",
      "epoch: 70 batch_id: 91 Batch loss: 0.5897678\n",
      "epoch: 70 batch_id: 92 Batch loss: 0.5042939\n",
      "epoch: 70 batch_id: 93 Batch loss: 0.5257644\n",
      "epoch: 70 batch_id: 94 Batch loss: 0.59411544\n",
      "epoch: 70 batch_id: 95 Batch loss: 0.53127074\n",
      "epoch: 70 batch_id: 96 Batch loss: 0.52793497\n",
      "epoch: 70 batch_id: 97 Batch loss: 0.45459992\n",
      "epoch: 70 batch_id: 98 Batch loss: 0.33325058\n",
      "epoch: 70 batch_id: 99 Batch loss: 0.52923644\n",
      "epoch: 70 batch_id: 100 Batch loss: 0.61351496\n",
      "epoch: 70 batch_id: 101 Batch loss: 0.45429662\n",
      "epoch: 70 batch_id: 102 Batch loss: 0.44828224\n",
      "epoch: 70 batch_id: 103 Batch loss: 0.45486283\n",
      "epoch: 70 batch_id: 104 Batch loss: 0.8064258\n",
      "epoch: 70 batch_id: 105 Batch loss: 0.5130744\n",
      "epoch: 70 batch_id: 106 Batch loss: 0.33917478\n",
      "epoch: 70 batch_id: 107 Batch loss: 0.4456274\n",
      "epoch: 70 batch_id: 108 Batch loss: 0.7035552\n",
      "epoch: 70 batch_id: 109 Batch loss: 0.5633413\n",
      "epoch: 70 batch_id: 110 Batch loss: 0.6703308\n",
      "epoch: 70 batch_id: 111 Batch loss: 0.6220169\n",
      "epoch: 70 batch_id: 112 Batch loss: 0.5272833\n",
      "epoch: 70 batch_id: 113 Batch loss: 0.43868935\n",
      "epoch: 70 batch_id: 114 Batch loss: 0.31749007\n",
      "epoch: 70 batch_id: 115 Batch loss: 0.6150554\n",
      "epoch: 70 batch_id: 116 Batch loss: 0.70694107\n",
      "epoch: 70 batch_id: 117 Batch loss: 0.5845261\n",
      "epoch: 70 batch_id: 118 Batch loss: 0.6845483\n",
      "epoch: 70 batch_id: 119 Batch loss: 0.51719874\n",
      "epoch: 70 batch_id: 120 Batch loss: 0.44984874\n",
      "epoch: 70 batch_id: 121 Batch loss: 0.5199928\n",
      "epoch: 70 batch_id: 122 Batch loss: 0.55732006\n",
      "epoch: 70 batch_id: 123 Batch loss: 0.5117917\n",
      "epoch: 70 batch_id: 124 Batch loss: 0.54373735\n",
      "epoch: 70 batch_id: 125 Batch loss: 0.48139656\n",
      "epoch: 70 batch_id: 126 Batch loss: 0.704993\n",
      "epoch: 70 batch_id: 127 Batch loss: 0.4699755\n",
      "epoch: 70 batch_id: 128 Batch loss: 0.49363217\n",
      "epoch: 70 batch_id: 129 Batch loss: 0.31487033\n",
      "epoch: 70 batch_id: 130 Batch loss: 0.6115042\n",
      "epoch: 70 batch_id: 131 Batch loss: 0.53598005\n",
      "epoch: 70 batch_id: 132 Batch loss: 0.514441\n",
      "epoch: 70 batch_id: 133 Batch loss: 0.61151034\n",
      "epoch: 70 batch_id: 134 Batch loss: 0.7095188\n",
      "epoch: 70 batch_id: 135 Batch loss: 0.5083406\n",
      "epoch: 70 batch_id: 136 Batch loss: 0.48515007\n",
      "epoch: 70 batch_id: 137 Batch loss: 0.73913974\n",
      "epoch: 70 batch_id: 138 Batch loss: 0.43593696\n",
      "epoch: 70 batch_id: 139 Batch loss: 0.44867036\n",
      "epoch: 70 batch_id: 140 Batch loss: 0.5496677\n",
      "epoch: 70 batch_id: 141 Batch loss: 0.55536896\n",
      "epoch: 70 batch_id: 142 Batch loss: 0.44797558\n",
      "epoch: 70 batch_id: 143 Batch loss: 0.39745983\n",
      "epoch: 70 batch_id: 144 Batch loss: 0.30446061\n",
      "epoch: 70 batch_id: 145 Batch loss: 0.8138354\n",
      "epoch: 70 batch_id: 146 Batch loss: 0.4579784\n",
      "epoch: 70 batch_id: 147 Batch loss: 0.4644751\n",
      "epoch: 70 batch_id: 148 Batch loss: 0.5252291\n",
      "epoch: 70 batch_id: 149 Batch loss: 0.38290542\n",
      "epoch: 70 batch_id: 150 Batch loss: 0.5764668\n",
      "epoch: 70 batch_id: 151 Batch loss: 0.44841358\n",
      "epoch: 70 batch_id: 152 Batch loss: 0.5905363\n",
      "epoch: 70 batch_id: 153 Batch loss: 0.5166992\n",
      "epoch: 70 batch_id: 154 Batch loss: 0.47928375\n",
      "epoch: 70 batch_id: 155 Batch loss: 0.5039303\n",
      "epoch: 70 batch_id: 156 Batch loss: 0.5674616\n",
      "epoch: 70 batch_id: 157 Batch loss: 0.49335575\n",
      "epoch: 70 batch_id: 158 Batch loss: 0.4122291\n",
      "epoch: 70 batch_id: 159 Batch loss: 0.59968203\n",
      "epoch: 70 batch_id: 160 Batch loss: 0.6634704\n",
      "epoch: 70 batch_id: 161 Batch loss: 0.56105196\n",
      "epoch: 70 batch_id: 162 Batch loss: 0.83335876\n",
      "epoch: 70 batch_id: 163 Batch loss: 0.39530832\n",
      "epoch: 70 batch_id: 164 Batch loss: 0.3803646\n",
      "epoch: 70 batch_id: 165 Batch loss: 0.4698945\n",
      "epoch: 70 batch_id: 166 Batch loss: 0.4487844\n",
      "epoch: 70 batch_id: 167 Batch loss: 0.51158243\n",
      "epoch: 70 batch_id: 168 Batch loss: 0.6682652\n",
      "epoch: 70 batch_id: 169 Batch loss: 0.5378989\n",
      "epoch: 70 batch_id: 170 Batch loss: 0.47125977\n",
      "epoch: 70 batch_id: 171 Batch loss: 0.6536712\n",
      "epoch: 70 batch_id: 172 Batch loss: 0.4398507\n",
      "epoch: 70 batch_id: 173 Batch loss: 0.67923427\n",
      "epoch: 70 batch_id: 174 Batch loss: 0.5943531\n",
      "epoch: 70 batch_id: 175 Batch loss: 0.6427708\n",
      "epoch: 70 batch_id: 176 Batch loss: 0.47224367\n",
      "epoch: 70 batch_id: 177 Batch loss: 0.58716714\n",
      "epoch: 70 batch_id: 178 Batch loss: 0.65581805\n",
      "epoch: 70 batch_id: 179 Batch loss: 0.66026384\n",
      "epoch: 70 batch_id: 180 Batch loss: 0.39168534\n",
      "epoch: 70 batch_id: 181 Batch loss: 0.68772185\n",
      "epoch: 70 batch_id: 182 Batch loss: 0.52185357\n",
      "epoch: 70 batch_id: 183 Batch loss: 0.3837899\n",
      "epoch: 70 batch_id: 184 Batch loss: 0.6611078\n",
      "epoch: 70 batch_id: 185 Batch loss: 0.73957354\n",
      "epoch: 70 batch_id: 186 Batch loss: 0.6908964\n",
      "epoch: 70 batch_id: 187 Batch loss: 0.4971992\n",
      "epoch: 70 batch_id: 188 Batch loss: 0.44657594\n",
      "epoch: 70 batch_id: 189 Batch loss: 0.53821856\n",
      "epoch: 70 batch_id: 190 Batch loss: 0.740529\n",
      "epoch: 70 batch_id: 191 Batch loss: 0.6975934\n",
      "epoch: 70 batch_id: 192 Batch loss: 0.6194416\n",
      "epoch: 70 batch_id: 193 Batch loss: 0.42522943\n",
      "epoch: 70 batch_id: 194 Batch loss: 0.5250536\n",
      "epoch: 70 batch_id: 195 Batch loss: 0.38309777\n",
      "epoch: 70 batch_id: 196 Batch loss: 0.60428554\n",
      "epoch: 70 batch_id: 197 Batch loss: 0.4484403\n",
      "epoch: 70 batch_id: 198 Batch loss: 0.41323578\n",
      "epoch: 70 batch_id: 199 Batch loss: 0.49321118\n",
      "epoch: 70 batch_id: 200 Batch loss: 0.43073264\n",
      "epoch: 70 batch_id: 201 Batch loss: 0.38119808\n",
      "epoch: 70 batch_id: 202 Batch loss: 0.44942662\n",
      "epoch: 70 batch_id: 203 Batch loss: 0.53770083\n",
      "epoch: 70 batch_id: 204 Batch loss: 0.48673606\n",
      "epoch: 70 batch_id: 205 Batch loss: 0.5069635\n",
      "epoch: 70 batch_id: 206 Batch loss: 0.4481215\n",
      "epoch: 80 batch_id: 0 Batch loss: 0.56398684\n",
      "epoch: 80 batch_id: 1 Batch loss: 0.38327843\n",
      "epoch: 80 batch_id: 2 Batch loss: 0.63135636\n",
      "epoch: 80 batch_id: 3 Batch loss: 0.64074296\n",
      "epoch: 80 batch_id: 4 Batch loss: 0.8682395\n",
      "epoch: 80 batch_id: 5 Batch loss: 0.6308996\n",
      "epoch: 80 batch_id: 6 Batch loss: 0.40372834\n",
      "epoch: 80 batch_id: 7 Batch loss: 0.47075745\n",
      "epoch: 80 batch_id: 8 Batch loss: 0.5361882\n",
      "epoch: 80 batch_id: 9 Batch loss: 0.5166576\n",
      "epoch: 80 batch_id: 10 Batch loss: 1.0786573\n",
      "epoch: 80 batch_id: 11 Batch loss: 0.618933\n",
      "epoch: 80 batch_id: 12 Batch loss: 0.33140007\n",
      "epoch: 80 batch_id: 13 Batch loss: 0.43405575\n",
      "epoch: 80 batch_id: 14 Batch loss: 0.5290424\n",
      "epoch: 80 batch_id: 15 Batch loss: 0.72546875\n",
      "epoch: 80 batch_id: 16 Batch loss: 0.50291306\n",
      "epoch: 80 batch_id: 17 Batch loss: 0.5607249\n",
      "epoch: 80 batch_id: 18 Batch loss: 0.6819786\n",
      "epoch: 80 batch_id: 19 Batch loss: 0.6076802\n",
      "epoch: 80 batch_id: 20 Batch loss: 0.44546327\n",
      "epoch: 80 batch_id: 21 Batch loss: 0.4650947\n",
      "epoch: 80 batch_id: 22 Batch loss: 0.4745884\n",
      "epoch: 80 batch_id: 23 Batch loss: 0.6108\n",
      "epoch: 80 batch_id: 24 Batch loss: 0.40244058\n",
      "epoch: 80 batch_id: 25 Batch loss: 0.6268576\n",
      "epoch: 80 batch_id: 26 Batch loss: 0.5246016\n",
      "epoch: 80 batch_id: 27 Batch loss: 0.484775\n",
      "epoch: 80 batch_id: 28 Batch loss: 0.7304143\n",
      "epoch: 80 batch_id: 29 Batch loss: 0.6249593\n",
      "epoch: 80 batch_id: 30 Batch loss: 0.604154\n",
      "epoch: 80 batch_id: 31 Batch loss: 0.6244156\n",
      "epoch: 80 batch_id: 32 Batch loss: 0.7083227\n",
      "epoch: 80 batch_id: 33 Batch loss: 0.4679165\n",
      "epoch: 80 batch_id: 34 Batch loss: 0.47830993\n",
      "epoch: 80 batch_id: 35 Batch loss: 0.4928196\n",
      "epoch: 80 batch_id: 36 Batch loss: 0.58298975\n",
      "epoch: 80 batch_id: 37 Batch loss: 0.40349364\n",
      "epoch: 80 batch_id: 38 Batch loss: 0.57131165\n",
      "epoch: 80 batch_id: 39 Batch loss: 0.531674\n",
      "epoch: 80 batch_id: 40 Batch loss: 0.54290086\n",
      "epoch: 80 batch_id: 41 Batch loss: 0.4613466\n",
      "epoch: 80 batch_id: 42 Batch loss: 0.42081848\n",
      "epoch: 80 batch_id: 43 Batch loss: 0.6061098\n",
      "epoch: 80 batch_id: 44 Batch loss: 0.6616201\n",
      "epoch: 80 batch_id: 45 Batch loss: 0.4551931\n",
      "epoch: 80 batch_id: 46 Batch loss: 0.48571613\n",
      "epoch: 80 batch_id: 47 Batch loss: 0.43379724\n",
      "epoch: 80 batch_id: 48 Batch loss: 0.4965538\n",
      "epoch: 80 batch_id: 49 Batch loss: 0.6054248\n",
      "epoch: 80 batch_id: 50 Batch loss: 0.81190646\n",
      "epoch: 80 batch_id: 51 Batch loss: 0.47650027\n",
      "epoch: 80 batch_id: 52 Batch loss: 0.7316446\n",
      "epoch: 80 batch_id: 53 Batch loss: 0.49986184\n",
      "epoch: 80 batch_id: 54 Batch loss: 0.5918235\n",
      "epoch: 80 batch_id: 55 Batch loss: 0.43849602\n",
      "epoch: 80 batch_id: 56 Batch loss: 0.6020524\n",
      "epoch: 80 batch_id: 57 Batch loss: 0.51901007\n",
      "epoch: 80 batch_id: 58 Batch loss: 0.4094715\n",
      "epoch: 80 batch_id: 59 Batch loss: 0.519191\n",
      "epoch: 80 batch_id: 60 Batch loss: 0.47200352\n",
      "epoch: 80 batch_id: 61 Batch loss: 0.54228437\n",
      "epoch: 80 batch_id: 62 Batch loss: 0.41692886\n",
      "epoch: 80 batch_id: 63 Batch loss: 0.37123758\n",
      "epoch: 80 batch_id: 64 Batch loss: 0.49872598\n",
      "epoch: 80 batch_id: 65 Batch loss: 0.5590652\n",
      "epoch: 80 batch_id: 66 Batch loss: 0.5444755\n",
      "epoch: 80 batch_id: 67 Batch loss: 0.55667424\n",
      "epoch: 80 batch_id: 68 Batch loss: 0.43177304\n",
      "epoch: 80 batch_id: 69 Batch loss: 0.4730342\n",
      "epoch: 80 batch_id: 70 Batch loss: 0.3574006\n",
      "epoch: 80 batch_id: 71 Batch loss: 0.504136\n",
      "epoch: 80 batch_id: 72 Batch loss: 0.65936774\n",
      "epoch: 80 batch_id: 73 Batch loss: 0.54270124\n",
      "epoch: 80 batch_id: 74 Batch loss: 0.4933057\n",
      "epoch: 80 batch_id: 75 Batch loss: 0.44255653\n",
      "epoch: 80 batch_id: 76 Batch loss: 0.47847396\n",
      "epoch: 80 batch_id: 77 Batch loss: 0.34604266\n",
      "epoch: 80 batch_id: 78 Batch loss: 0.8095405\n",
      "epoch: 80 batch_id: 79 Batch loss: 0.5193905\n",
      "epoch: 80 batch_id: 80 Batch loss: 0.5682556\n",
      "epoch: 80 batch_id: 81 Batch loss: 0.5751635\n",
      "epoch: 80 batch_id: 82 Batch loss: 0.50083005\n",
      "epoch: 80 batch_id: 83 Batch loss: 0.4853883\n",
      "epoch: 80 batch_id: 84 Batch loss: 0.4729283\n",
      "epoch: 80 batch_id: 85 Batch loss: 0.80614305\n",
      "epoch: 80 batch_id: 86 Batch loss: 0.45893824\n",
      "epoch: 80 batch_id: 87 Batch loss: 0.44519675\n",
      "epoch: 80 batch_id: 88 Batch loss: 0.4785565\n",
      "epoch: 80 batch_id: 89 Batch loss: 0.6522673\n",
      "epoch: 80 batch_id: 90 Batch loss: 0.53218126\n",
      "epoch: 80 batch_id: 91 Batch loss: 0.5409323\n",
      "epoch: 80 batch_id: 92 Batch loss: 0.64605844\n",
      "epoch: 80 batch_id: 93 Batch loss: 0.5404471\n",
      "epoch: 80 batch_id: 94 Batch loss: 0.5012555\n",
      "epoch: 80 batch_id: 95 Batch loss: 0.4328266\n",
      "epoch: 80 batch_id: 96 Batch loss: 0.5086108\n",
      "epoch: 80 batch_id: 97 Batch loss: 0.63013196\n",
      "epoch: 80 batch_id: 98 Batch loss: 0.382332\n",
      "epoch: 80 batch_id: 99 Batch loss: 0.36728394\n",
      "epoch: 80 batch_id: 100 Batch loss: 0.48286003\n",
      "epoch: 80 batch_id: 101 Batch loss: 0.57156783\n",
      "epoch: 80 batch_id: 102 Batch loss: 0.52868104\n",
      "epoch: 80 batch_id: 103 Batch loss: 0.63580704\n",
      "epoch: 80 batch_id: 104 Batch loss: 0.37284547\n",
      "epoch: 80 batch_id: 105 Batch loss: 0.51290464\n",
      "epoch: 80 batch_id: 106 Batch loss: 0.62799543\n",
      "epoch: 80 batch_id: 107 Batch loss: 0.3213727\n",
      "epoch: 80 batch_id: 108 Batch loss: 0.6317592\n",
      "epoch: 80 batch_id: 109 Batch loss: 0.771126\n",
      "epoch: 80 batch_id: 110 Batch loss: 0.5393943\n",
      "epoch: 80 batch_id: 111 Batch loss: 0.46132928\n",
      "epoch: 80 batch_id: 112 Batch loss: 0.43132728\n",
      "epoch: 80 batch_id: 113 Batch loss: 0.6136343\n",
      "epoch: 80 batch_id: 114 Batch loss: 0.46012077\n",
      "epoch: 80 batch_id: 115 Batch loss: 0.53025335\n",
      "epoch: 80 batch_id: 116 Batch loss: 0.44236016\n",
      "epoch: 80 batch_id: 117 Batch loss: 0.7254229\n",
      "epoch: 80 batch_id: 118 Batch loss: 0.590185\n",
      "epoch: 80 batch_id: 119 Batch loss: 0.4607837\n",
      "epoch: 80 batch_id: 120 Batch loss: 0.647089\n",
      "epoch: 80 batch_id: 121 Batch loss: 0.41436726\n",
      "epoch: 80 batch_id: 122 Batch loss: 0.44547898\n",
      "epoch: 80 batch_id: 123 Batch loss: 0.27829468\n",
      "epoch: 80 batch_id: 124 Batch loss: 0.4155812\n",
      "epoch: 80 batch_id: 125 Batch loss: 0.4844067\n",
      "epoch: 80 batch_id: 126 Batch loss: 0.6194716\n",
      "epoch: 80 batch_id: 127 Batch loss: 0.4436294\n",
      "epoch: 80 batch_id: 128 Batch loss: 0.54895884\n",
      "epoch: 80 batch_id: 129 Batch loss: 0.48410404\n",
      "epoch: 80 batch_id: 130 Batch loss: 0.6066632\n",
      "epoch: 80 batch_id: 131 Batch loss: 0.49388337\n",
      "epoch: 80 batch_id: 132 Batch loss: 0.39674103\n",
      "epoch: 80 batch_id: 133 Batch loss: 0.62398696\n",
      "epoch: 80 batch_id: 134 Batch loss: 0.42298707\n",
      "epoch: 80 batch_id: 135 Batch loss: 0.48105034\n",
      "epoch: 80 batch_id: 136 Batch loss: 0.4830439\n",
      "epoch: 80 batch_id: 137 Batch loss: 0.4533495\n",
      "epoch: 80 batch_id: 138 Batch loss: 0.5297364\n",
      "epoch: 80 batch_id: 139 Batch loss: 0.53034365\n",
      "epoch: 80 batch_id: 140 Batch loss: 0.48756093\n",
      "epoch: 80 batch_id: 141 Batch loss: 0.52365255\n",
      "epoch: 80 batch_id: 142 Batch loss: 0.3891813\n",
      "epoch: 80 batch_id: 143 Batch loss: 0.6573628\n",
      "epoch: 80 batch_id: 144 Batch loss: 0.3400074\n",
      "epoch: 80 batch_id: 145 Batch loss: 0.67574394\n",
      "epoch: 80 batch_id: 146 Batch loss: 0.48474663\n",
      "epoch: 80 batch_id: 147 Batch loss: 0.4925492\n",
      "epoch: 80 batch_id: 148 Batch loss: 0.4544304\n",
      "epoch: 80 batch_id: 149 Batch loss: 0.46386573\n",
      "epoch: 80 batch_id: 150 Batch loss: 0.30804223\n",
      "epoch: 80 batch_id: 151 Batch loss: 0.43053773\n",
      "epoch: 80 batch_id: 152 Batch loss: 0.48464853\n",
      "epoch: 80 batch_id: 153 Batch loss: 0.4315037\n",
      "epoch: 80 batch_id: 154 Batch loss: 0.44725713\n",
      "epoch: 80 batch_id: 155 Batch loss: 0.3030257\n",
      "epoch: 80 batch_id: 156 Batch loss: 0.4430784\n",
      "epoch: 80 batch_id: 157 Batch loss: 0.5082965\n",
      "epoch: 80 batch_id: 158 Batch loss: 0.4967886\n",
      "epoch: 80 batch_id: 159 Batch loss: 0.46852598\n",
      "epoch: 80 batch_id: 160 Batch loss: 0.42986938\n",
      "epoch: 80 batch_id: 161 Batch loss: 0.6591951\n",
      "epoch: 80 batch_id: 162 Batch loss: 0.64953977\n",
      "epoch: 80 batch_id: 163 Batch loss: 0.5787035\n",
      "epoch: 80 batch_id: 164 Batch loss: 0.67861074\n",
      "epoch: 80 batch_id: 165 Batch loss: 0.7780795\n",
      "epoch: 80 batch_id: 166 Batch loss: 0.5765103\n",
      "epoch: 80 batch_id: 167 Batch loss: 0.5313764\n",
      "epoch: 80 batch_id: 168 Batch loss: 0.59218544\n",
      "epoch: 80 batch_id: 169 Batch loss: 0.41894472\n",
      "epoch: 80 batch_id: 170 Batch loss: 0.41148436\n",
      "epoch: 80 batch_id: 171 Batch loss: 0.45872673\n",
      "epoch: 80 batch_id: 172 Batch loss: 0.5464139\n",
      "epoch: 80 batch_id: 173 Batch loss: 0.83154833\n",
      "epoch: 80 batch_id: 174 Batch loss: 0.49625885\n",
      "epoch: 80 batch_id: 175 Batch loss: 1.4350847\n",
      "epoch: 80 batch_id: 176 Batch loss: 0.41526145\n",
      "epoch: 80 batch_id: 177 Batch loss: 0.6221081\n",
      "epoch: 80 batch_id: 178 Batch loss: 0.64961106\n",
      "epoch: 80 batch_id: 179 Batch loss: 0.40889388\n",
      "epoch: 80 batch_id: 180 Batch loss: 0.4497836\n",
      "epoch: 80 batch_id: 181 Batch loss: 0.3333791\n",
      "epoch: 80 batch_id: 182 Batch loss: 0.3729567\n",
      "epoch: 80 batch_id: 183 Batch loss: 0.74325544\n",
      "epoch: 80 batch_id: 184 Batch loss: 0.7231015\n",
      "epoch: 80 batch_id: 185 Batch loss: 0.45779395\n",
      "epoch: 80 batch_id: 186 Batch loss: 0.41871178\n",
      "epoch: 80 batch_id: 187 Batch loss: 0.5215287\n",
      "epoch: 80 batch_id: 188 Batch loss: 0.57792896\n",
      "epoch: 80 batch_id: 189 Batch loss: 0.54601413\n",
      "epoch: 80 batch_id: 190 Batch loss: 0.5898422\n",
      "epoch: 80 batch_id: 191 Batch loss: 0.4520554\n",
      "epoch: 80 batch_id: 192 Batch loss: 0.84445554\n",
      "epoch: 80 batch_id: 193 Batch loss: 0.4637932\n",
      "epoch: 80 batch_id: 194 Batch loss: 0.5385563\n",
      "epoch: 80 batch_id: 195 Batch loss: 0.45406792\n",
      "epoch: 80 batch_id: 196 Batch loss: 0.37185353\n",
      "epoch: 80 batch_id: 197 Batch loss: 0.59669137\n",
      "epoch: 80 batch_id: 198 Batch loss: 0.43833005\n",
      "epoch: 80 batch_id: 199 Batch loss: 0.64895403\n",
      "epoch: 80 batch_id: 200 Batch loss: 0.77831817\n",
      "epoch: 80 batch_id: 201 Batch loss: 0.5188962\n",
      "epoch: 80 batch_id: 202 Batch loss: 0.5224718\n",
      "epoch: 80 batch_id: 203 Batch loss: 0.3536664\n",
      "epoch: 80 batch_id: 204 Batch loss: 0.50717276\n",
      "epoch: 80 batch_id: 205 Batch loss: 0.8983288\n",
      "epoch: 80 batch_id: 206 Batch loss: 0.5856123\n",
      "epoch: 90 batch_id: 0 Batch loss: 0.52538186\n",
      "epoch: 90 batch_id: 1 Batch loss: 0.571651\n",
      "epoch: 90 batch_id: 2 Batch loss: 0.4898928\n",
      "epoch: 90 batch_id: 3 Batch loss: 0.8357445\n",
      "epoch: 90 batch_id: 4 Batch loss: 0.48098466\n",
      "epoch: 90 batch_id: 5 Batch loss: 0.5060827\n",
      "epoch: 90 batch_id: 6 Batch loss: 0.4705181\n",
      "epoch: 90 batch_id: 7 Batch loss: 0.5537347\n",
      "epoch: 90 batch_id: 8 Batch loss: 0.44850415\n",
      "epoch: 90 batch_id: 9 Batch loss: 0.50108415\n",
      "epoch: 90 batch_id: 10 Batch loss: 0.68887067\n",
      "epoch: 90 batch_id: 11 Batch loss: 0.50911313\n",
      "epoch: 90 batch_id: 12 Batch loss: 0.46498343\n",
      "epoch: 90 batch_id: 13 Batch loss: 0.47256124\n",
      "epoch: 90 batch_id: 14 Batch loss: 0.48763686\n",
      "epoch: 90 batch_id: 15 Batch loss: 0.5656551\n",
      "epoch: 90 batch_id: 16 Batch loss: 0.50732535\n",
      "epoch: 90 batch_id: 17 Batch loss: 0.45008567\n",
      "epoch: 90 batch_id: 18 Batch loss: 0.9579045\n",
      "epoch: 90 batch_id: 19 Batch loss: 0.7227844\n",
      "epoch: 90 batch_id: 20 Batch loss: 0.39092606\n",
      "epoch: 90 batch_id: 21 Batch loss: 0.4656002\n",
      "epoch: 90 batch_id: 22 Batch loss: 0.48383582\n",
      "epoch: 90 batch_id: 23 Batch loss: 0.4459159\n",
      "epoch: 90 batch_id: 24 Batch loss: 0.4116708\n",
      "epoch: 90 batch_id: 25 Batch loss: 0.395558\n",
      "epoch: 90 batch_id: 26 Batch loss: 0.46335617\n",
      "epoch: 90 batch_id: 27 Batch loss: 0.5882114\n",
      "epoch: 90 batch_id: 28 Batch loss: 0.7332227\n",
      "epoch: 90 batch_id: 29 Batch loss: 0.5975951\n",
      "epoch: 90 batch_id: 30 Batch loss: 0.5105601\n",
      "epoch: 90 batch_id: 31 Batch loss: 0.5190343\n",
      "epoch: 90 batch_id: 32 Batch loss: 0.5603697\n",
      "epoch: 90 batch_id: 33 Batch loss: 0.39951745\n",
      "epoch: 90 batch_id: 34 Batch loss: 0.60120547\n",
      "epoch: 90 batch_id: 35 Batch loss: 0.49229652\n",
      "epoch: 90 batch_id: 36 Batch loss: 0.54351884\n",
      "epoch: 90 batch_id: 37 Batch loss: 0.5074916\n",
      "epoch: 90 batch_id: 38 Batch loss: 0.5046675\n",
      "epoch: 90 batch_id: 39 Batch loss: 0.35445535\n",
      "epoch: 90 batch_id: 40 Batch loss: 0.5034874\n",
      "epoch: 90 batch_id: 41 Batch loss: 0.36281124\n",
      "epoch: 90 batch_id: 42 Batch loss: 0.46548405\n",
      "epoch: 90 batch_id: 43 Batch loss: 0.516709\n",
      "epoch: 90 batch_id: 44 Batch loss: 0.56129247\n",
      "epoch: 90 batch_id: 45 Batch loss: 0.6236533\n",
      "epoch: 90 batch_id: 46 Batch loss: 0.48330453\n",
      "epoch: 90 batch_id: 47 Batch loss: 0.6570058\n",
      "epoch: 90 batch_id: 48 Batch loss: 0.47131807\n",
      "epoch: 90 batch_id: 49 Batch loss: 0.6470566\n",
      "epoch: 90 batch_id: 50 Batch loss: 0.4975296\n",
      "epoch: 90 batch_id: 51 Batch loss: 0.5677948\n",
      "epoch: 90 batch_id: 52 Batch loss: 0.7167537\n",
      "epoch: 90 batch_id: 53 Batch loss: 0.4892386\n",
      "epoch: 90 batch_id: 54 Batch loss: 0.7533234\n",
      "epoch: 90 batch_id: 55 Batch loss: 0.52149415\n",
      "epoch: 90 batch_id: 56 Batch loss: 0.3533483\n",
      "epoch: 90 batch_id: 57 Batch loss: 0.53693694\n",
      "epoch: 90 batch_id: 58 Batch loss: 0.7413831\n",
      "epoch: 90 batch_id: 59 Batch loss: 0.4940786\n",
      "epoch: 90 batch_id: 60 Batch loss: 0.5789131\n",
      "epoch: 90 batch_id: 61 Batch loss: 0.6088388\n",
      "epoch: 90 batch_id: 62 Batch loss: 0.32111245\n",
      "epoch: 90 batch_id: 63 Batch loss: 0.3918258\n",
      "epoch: 90 batch_id: 64 Batch loss: 0.6751512\n",
      "epoch: 90 batch_id: 65 Batch loss: 0.49269912\n",
      "epoch: 90 batch_id: 66 Batch loss: 0.41413662\n",
      "epoch: 90 batch_id: 67 Batch loss: 0.48771453\n",
      "epoch: 90 batch_id: 68 Batch loss: 0.46065414\n",
      "epoch: 90 batch_id: 69 Batch loss: 0.60753983\n",
      "epoch: 90 batch_id: 70 Batch loss: 0.42521894\n",
      "epoch: 90 batch_id: 71 Batch loss: 0.5152813\n",
      "epoch: 90 batch_id: 72 Batch loss: 0.64669067\n",
      "epoch: 90 batch_id: 73 Batch loss: 0.5337719\n",
      "epoch: 90 batch_id: 74 Batch loss: 0.8137323\n",
      "epoch: 90 batch_id: 75 Batch loss: 0.40085724\n",
      "epoch: 90 batch_id: 76 Batch loss: 0.36033645\n",
      "epoch: 90 batch_id: 77 Batch loss: 0.41581482\n",
      "epoch: 90 batch_id: 78 Batch loss: 0.46032012\n",
      "epoch: 90 batch_id: 79 Batch loss: 0.39355043\n",
      "epoch: 90 batch_id: 80 Batch loss: 0.31798774\n",
      "epoch: 90 batch_id: 81 Batch loss: 0.44453323\n",
      "epoch: 90 batch_id: 82 Batch loss: 0.47428468\n",
      "epoch: 90 batch_id: 83 Batch loss: 0.5085461\n",
      "epoch: 90 batch_id: 84 Batch loss: 0.52744573\n",
      "epoch: 90 batch_id: 85 Batch loss: 0.6101672\n",
      "epoch: 90 batch_id: 86 Batch loss: 0.36702058\n",
      "epoch: 90 batch_id: 87 Batch loss: 0.44353318\n",
      "epoch: 90 batch_id: 88 Batch loss: 0.46916717\n",
      "epoch: 90 batch_id: 89 Batch loss: 0.48423898\n",
      "epoch: 90 batch_id: 90 Batch loss: 0.6112646\n",
      "epoch: 90 batch_id: 91 Batch loss: 0.53245866\n",
      "epoch: 90 batch_id: 92 Batch loss: 0.5384434\n",
      "epoch: 90 batch_id: 93 Batch loss: 0.59166616\n",
      "epoch: 90 batch_id: 94 Batch loss: 0.45846602\n",
      "epoch: 90 batch_id: 95 Batch loss: 0.6334299\n",
      "epoch: 90 batch_id: 96 Batch loss: 1.0401634\n",
      "epoch: 90 batch_id: 97 Batch loss: 0.62730515\n",
      "epoch: 90 batch_id: 98 Batch loss: 0.72370857\n",
      "epoch: 90 batch_id: 99 Batch loss: 0.6832434\n",
      "epoch: 90 batch_id: 100 Batch loss: 0.78486794\n",
      "epoch: 90 batch_id: 101 Batch loss: 0.39718398\n",
      "epoch: 90 batch_id: 102 Batch loss: 0.52313745\n",
      "epoch: 90 batch_id: 103 Batch loss: 0.5974202\n",
      "epoch: 90 batch_id: 104 Batch loss: 0.5426255\n",
      "epoch: 90 batch_id: 105 Batch loss: 0.35917693\n",
      "epoch: 90 batch_id: 106 Batch loss: 0.61245537\n",
      "epoch: 90 batch_id: 107 Batch loss: 0.57696414\n",
      "epoch: 90 batch_id: 108 Batch loss: 0.4969726\n",
      "epoch: 90 batch_id: 109 Batch loss: 0.7606846\n",
      "epoch: 90 batch_id: 110 Batch loss: 0.5268054\n",
      "epoch: 90 batch_id: 111 Batch loss: 0.5013779\n",
      "epoch: 90 batch_id: 112 Batch loss: 0.47882858\n",
      "epoch: 90 batch_id: 113 Batch loss: 0.38540122\n",
      "epoch: 90 batch_id: 114 Batch loss: 0.5110309\n",
      "epoch: 90 batch_id: 115 Batch loss: 0.522379\n",
      "epoch: 90 batch_id: 116 Batch loss: 0.40051487\n",
      "epoch: 90 batch_id: 117 Batch loss: 0.47452998\n",
      "epoch: 90 batch_id: 118 Batch loss: 0.50073254\n",
      "epoch: 90 batch_id: 119 Batch loss: 0.7442921\n",
      "epoch: 90 batch_id: 120 Batch loss: 0.62207824\n",
      "epoch: 90 batch_id: 121 Batch loss: 0.6661935\n",
      "epoch: 90 batch_id: 122 Batch loss: 0.6033238\n",
      "epoch: 90 batch_id: 123 Batch loss: 0.63870347\n",
      "epoch: 90 batch_id: 124 Batch loss: 0.42658547\n",
      "epoch: 90 batch_id: 125 Batch loss: 0.9053461\n",
      "epoch: 90 batch_id: 126 Batch loss: 0.63032603\n",
      "epoch: 90 batch_id: 127 Batch loss: 0.47287977\n",
      "epoch: 90 batch_id: 128 Batch loss: 0.46640968\n",
      "epoch: 90 batch_id: 129 Batch loss: 0.44344825\n",
      "epoch: 90 batch_id: 130 Batch loss: 0.49697274\n",
      "epoch: 90 batch_id: 131 Batch loss: 0.57178545\n",
      "epoch: 90 batch_id: 132 Batch loss: 0.3989914\n",
      "epoch: 90 batch_id: 133 Batch loss: 0.4597744\n",
      "epoch: 90 batch_id: 134 Batch loss: 0.38525558\n",
      "epoch: 90 batch_id: 135 Batch loss: 0.47133428\n",
      "epoch: 90 batch_id: 136 Batch loss: 0.5047323\n",
      "epoch: 90 batch_id: 137 Batch loss: 0.47852316\n",
      "epoch: 90 batch_id: 138 Batch loss: 0.4361418\n",
      "epoch: 90 batch_id: 139 Batch loss: 0.64756507\n",
      "epoch: 90 batch_id: 140 Batch loss: 0.40900078\n",
      "epoch: 90 batch_id: 141 Batch loss: 0.48704773\n",
      "epoch: 90 batch_id: 142 Batch loss: 0.9053377\n",
      "epoch: 90 batch_id: 143 Batch loss: 0.40269047\n",
      "epoch: 90 batch_id: 144 Batch loss: 0.59258264\n",
      "epoch: 90 batch_id: 145 Batch loss: 0.5195487\n",
      "epoch: 90 batch_id: 146 Batch loss: 0.52701575\n",
      "epoch: 90 batch_id: 147 Batch loss: 0.5076632\n",
      "epoch: 90 batch_id: 148 Batch loss: 0.4269653\n",
      "epoch: 90 batch_id: 149 Batch loss: 0.9980553\n",
      "epoch: 90 batch_id: 150 Batch loss: 0.43207002\n",
      "epoch: 90 batch_id: 151 Batch loss: 0.43270552\n",
      "epoch: 90 batch_id: 152 Batch loss: 0.56332946\n",
      "epoch: 90 batch_id: 153 Batch loss: 0.5951262\n",
      "epoch: 90 batch_id: 154 Batch loss: 0.5656005\n",
      "epoch: 90 batch_id: 155 Batch loss: 0.51878047\n",
      "epoch: 90 batch_id: 156 Batch loss: 0.45187593\n",
      "epoch: 90 batch_id: 157 Batch loss: 0.44698185\n",
      "epoch: 90 batch_id: 158 Batch loss: 0.4595331\n",
      "epoch: 90 batch_id: 159 Batch loss: 0.56058145\n",
      "epoch: 90 batch_id: 160 Batch loss: 0.5899057\n",
      "epoch: 90 batch_id: 161 Batch loss: 0.4006627\n",
      "epoch: 90 batch_id: 162 Batch loss: 0.5391043\n",
      "epoch: 90 batch_id: 163 Batch loss: 0.47233415\n",
      "epoch: 90 batch_id: 164 Batch loss: 0.5378452\n",
      "epoch: 90 batch_id: 165 Batch loss: 0.43823028\n",
      "epoch: 90 batch_id: 166 Batch loss: 0.63839793\n",
      "epoch: 90 batch_id: 167 Batch loss: 0.44373512\n",
      "epoch: 90 batch_id: 168 Batch loss: 0.5579438\n",
      "epoch: 90 batch_id: 169 Batch loss: 0.599213\n",
      "epoch: 90 batch_id: 170 Batch loss: 0.5439263\n",
      "epoch: 90 batch_id: 171 Batch loss: 0.5901735\n",
      "epoch: 90 batch_id: 172 Batch loss: 0.65638596\n",
      "epoch: 90 batch_id: 173 Batch loss: 0.47690147\n",
      "epoch: 90 batch_id: 174 Batch loss: 0.40495774\n",
      "epoch: 90 batch_id: 175 Batch loss: 0.54068977\n",
      "epoch: 90 batch_id: 176 Batch loss: 0.61697656\n",
      "epoch: 90 batch_id: 177 Batch loss: 0.5008929\n",
      "epoch: 90 batch_id: 178 Batch loss: 0.5276362\n",
      "epoch: 90 batch_id: 179 Batch loss: 0.5261371\n",
      "epoch: 90 batch_id: 180 Batch loss: 0.49152356\n",
      "epoch: 90 batch_id: 181 Batch loss: 0.528564\n",
      "epoch: 90 batch_id: 182 Batch loss: 0.54419446\n",
      "epoch: 90 batch_id: 183 Batch loss: 0.5927398\n",
      "epoch: 90 batch_id: 184 Batch loss: 0.5024353\n",
      "epoch: 90 batch_id: 185 Batch loss: 0.37320474\n",
      "epoch: 90 batch_id: 186 Batch loss: 0.61517\n",
      "epoch: 90 batch_id: 187 Batch loss: 0.6119251\n",
      "epoch: 90 batch_id: 188 Batch loss: 0.7520899\n",
      "epoch: 90 batch_id: 189 Batch loss: 0.70186317\n",
      "epoch: 90 batch_id: 190 Batch loss: 0.4725537\n",
      "epoch: 90 batch_id: 191 Batch loss: 0.5325267\n",
      "epoch: 90 batch_id: 192 Batch loss: 0.4457885\n",
      "epoch: 90 batch_id: 193 Batch loss: 0.57254976\n",
      "epoch: 90 batch_id: 194 Batch loss: 0.78368574\n",
      "epoch: 90 batch_id: 195 Batch loss: 0.633606\n",
      "epoch: 90 batch_id: 196 Batch loss: 0.51159376\n",
      "epoch: 90 batch_id: 197 Batch loss: 0.39703003\n",
      "epoch: 90 batch_id: 198 Batch loss: 0.38926545\n",
      "epoch: 90 batch_id: 199 Batch loss: 0.40655708\n",
      "epoch: 90 batch_id: 200 Batch loss: 0.5451481\n",
      "epoch: 90 batch_id: 201 Batch loss: 0.5662018\n",
      "epoch: 90 batch_id: 202 Batch loss: 0.56733686\n",
      "epoch: 90 batch_id: 203 Batch loss: 0.64033645\n",
      "epoch: 90 batch_id: 204 Batch loss: 0.4531557\n",
      "epoch: 90 batch_id: 205 Batch loss: 0.552844\n",
      "epoch: 90 batch_id: 206 Batch loss: 0.38659614\n"
     ]
    }
   ],
   "source": [
    "# 使用mini-batch方式\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "# 与上文例子相同，这里采用的是使用optimizer计算梯度\n",
    "n_epochs = 100\n",
    "display_step = 10\n",
    "learning_rate = 0.01\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\") #X0\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "# 定义mini-batch取数据方式\n",
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    np.random.seed(epoch * n_batches + batch_index) \n",
    "    indices = np.random.randint(m, size=batch_size)  \n",
    "    X_batch = data[indices] \n",
    "    y_batch = housing.target.reshape(-1, 1)[indices] \n",
    "    return X_batch, y_batch\n",
    "\n",
    "# mini-batch计算过程\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):#迭代的次数\n",
    "        avg_cost = 0.\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                total_loss = 0\n",
    "                acc_train = mse.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                total_loss += acc_train \n",
    "                #print acc_train,total_loss\n",
    "                print(\"epoch:\",epoch, \"batch_id:\",batch_index, \"Batch loss:\", acc_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T11:01:13.567845300Z",
     "start_time": "2024-02-28T11:01:09.907500500Z"
    }
   },
   "id": "edb1153ac58c8e71"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# 模型的保存和恢复\n",
    "* 在创建图阶段创建一个Saver的节点，在执行阶段需要保存模型的地方调用Save()函数即可\n",
    "* saver = tf.train.Saver()\n",
    "* save_path = saver.save(sess, \"model/model.ckpt\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7490fcba1262b9b1"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'X_8' with dtype float and shape [?,9]\n\t [[node X_8 (defined at C:\\Users\\huanyizhiyuan\\AppData\\Local\\Temp\\ipykernel_32524\\966860098.py:2) ]]\n\nOriginal stack trace for 'X_8':\n  File \"D:\\Anaconda\\envs\\tensor_test_1x\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"D:\\Anaconda\\envs\\tensor_test_1x\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"D:\\Anaconda\\envs\\tensor_test_1x\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n    app.launch_new_instance()\n  File \"D:\\Anaconda\\envs\\tensor_test_1x\\lib\\site-packages\\traitlets\\config\\application.py\", line 1043, in launch_instance\n    app.start()\n  File \"D:\\Anaconda\\envs\\tensor_test_1x\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n    self.io_loop.start()\n  File \"D:\\Anaconda\\envs\\tensor_test_1x\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n    self.asyncio_loop.run_forever()\n  File \"D:\\Anaconda\\envs\\tensor_test_1x\\lib\\asyncio\\base_events.py\", line 541, in run_forever\n    self._run_once()\n  File \"D:\\Anaconda\\envs\\tensor_test_1x\\lib\\asyncio\\base_events.py\", line 1786, in _run_once\n    handle._run()\n  File \"D:\\Anaconda\\envs\\tensor_test_1x\\lib\\asyncio\\events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"D:\\Anaconda\\envs\\tensor_test_1x\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n    await self.process_one()\n  File \"D:\\Anaconda\\envs\\tensor_test_1x\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n    await dispatch(*args)\n  File \"D:\\Anaconda\\envs\\tensor_test_1x\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n    await result\n  File \"D:\\Anaconda\\envs\\tensor_test_1x\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n    reply_content = await reply_content\n  File \"D:\\Anaconda\\envs\\tensor_test_1x\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 387, in do_execute\n    cell_id=cell_id,\n  File \"D:\\Anaconda\\envs\\tensor_test_1x\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"D:\\Anaconda\\envs\\tensor_test_1x\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2976, in run_cell\n    raw_cell, store_history, silent, shell_futures, cell_id\n  File \"D:\\Anaconda\\envs\\tensor_test_1x\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3030, in _run_cell\n    return runner(coro)\n  File \"D:\\Anaconda\\envs\\tensor_test_1x\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 78, in _pseudo_sync_runner\n    coro.send(None)\n  File \"D:\\Anaconda\\envs\\tensor_test_1x\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3258, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"D:\\Anaconda\\envs\\tensor_test_1x\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3473, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"D:\\Anaconda\\envs\\tensor_test_1x\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3553, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"C:\\Users\\huanyizhiyuan\\AppData\\Local\\Temp\\ipykernel_32524\\966860098.py\", line 2, in <module>\n    X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n  File \"D:\\Anaconda\\envs\\tensor_test_1x\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 2143, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"D:\\Anaconda\\envs\\tensor_test_1x\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 6261, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"D:\\Anaconda\\envs\\tensor_test_1x\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"D:\\Anaconda\\envs\\tensor_test_1x\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"D:\\Anaconda\\envs\\tensor_test_1x\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"D:\\Anaconda\\envs\\tensor_test_1x\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mInvalidArgumentError\u001B[0m                      Traceback (most recent call last)",
      "\u001B[1;32mD:\\Anaconda\\envs\\tensor_test_1x\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001B[0m in \u001B[0;36m_do_call\u001B[1;34m(self, fn, *args)\u001B[0m\n\u001B[0;32m   1355\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1356\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1357\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0merrors\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mOpError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda\\envs\\tensor_test_1x\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001B[0m in \u001B[0;36m_run_fn\u001B[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001B[0m\n\u001B[0;32m   1340\u001B[0m       return self._call_tf_sessionrun(\n\u001B[1;32m-> 1341\u001B[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001B[0m\u001B[0;32m   1342\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda\\envs\\tensor_test_1x\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001B[0m in \u001B[0;36m_call_tf_sessionrun\u001B[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001B[0m\n\u001B[0;32m   1428\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_session\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moptions\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfeed_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfetch_list\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtarget_list\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1429\u001B[1;33m         run_metadata)\n\u001B[0m\u001B[0;32m   1430\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mInvalidArgumentError\u001B[0m: You must feed a value for placeholder tensor 'X_8' with dtype float and shape [?,9]\n\t [[{{node X_8}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mInvalidArgumentError\u001B[0m                      Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_32524\\3044970826.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      6\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mepoch\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mn_epochs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mepoch\u001B[0m \u001B[1;33m%\u001B[0m \u001B[1;36m100\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 8\u001B[1;33m             \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Epoch\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mepoch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"MSE =\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmse\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0meval\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# 保存运行过程\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      9\u001B[0m             \u001B[0msave_path\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msaver\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msave\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msess\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"/my_model.ckpt\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m         \u001B[0msess\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtraining_op\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda\\envs\\tensor_test_1x\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001B[0m in \u001B[0;36meval\u001B[1;34m(self, feed_dict, session)\u001B[0m\n\u001B[0;32m    729\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    730\u001B[0m     \"\"\"\n\u001B[1;32m--> 731\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0m_eval_using_default_session\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfeed_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgraph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msession\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    732\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    733\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda\\envs\\tensor_test_1x\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001B[0m in \u001B[0;36m_eval_using_default_session\u001B[1;34m(tensors, feed_dict, graph, session)\u001B[0m\n\u001B[0;32m   5577\u001B[0m                        \u001B[1;34m\"the tensor's graph is different from the session's \"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   5578\u001B[0m                        \"graph.\")\n\u001B[1;32m-> 5579\u001B[1;33m   \u001B[1;32mreturn\u001B[0m \u001B[0msession\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtensors\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfeed_dict\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   5580\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   5581\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda\\envs\\tensor_test_1x\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001B[0m in \u001B[0;36mrun\u001B[1;34m(self, fetches, feed_dict, options, run_metadata)\u001B[0m\n\u001B[0;32m    948\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    949\u001B[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001B[1;32m--> 950\u001B[1;33m                          run_metadata_ptr)\n\u001B[0m\u001B[0;32m    951\u001B[0m       \u001B[1;32mif\u001B[0m \u001B[0mrun_metadata\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    952\u001B[0m         \u001B[0mproto_data\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtf_session\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTF_GetBuffer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrun_metadata_ptr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda\\envs\\tensor_test_1x\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001B[0m in \u001B[0;36m_run\u001B[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001B[0m\n\u001B[0;32m   1171\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mfinal_fetches\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mfinal_targets\u001B[0m \u001B[1;32mor\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mhandle\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mfeed_dict_tensor\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1172\u001B[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001B[1;32m-> 1173\u001B[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001B[0m\u001B[0;32m   1174\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1175\u001B[0m       \u001B[0mresults\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda\\envs\\tensor_test_1x\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001B[0m in \u001B[0;36m_do_run\u001B[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001B[0m\n\u001B[0;32m   1348\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mhandle\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1349\u001B[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001B[1;32m-> 1350\u001B[1;33m                            run_metadata)\n\u001B[0m\u001B[0;32m   1351\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1352\u001B[0m       \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_do_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m_prun_fn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhandle\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfeeds\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfetches\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda\\envs\\tensor_test_1x\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001B[0m in \u001B[0;36m_do_call\u001B[1;34m(self, fn, *args)\u001B[0m\n\u001B[0;32m   1368\u001B[0m           \u001B[1;32mpass\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1369\u001B[0m       \u001B[0mmessage\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0merror_interpolation\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minterpolate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmessage\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_graph\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1370\u001B[1;33m       \u001B[1;32mraise\u001B[0m \u001B[0mtype\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnode_def\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mop\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmessage\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1371\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1372\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0m_extend_graph\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mInvalidArgumentError\u001B[0m: You must feed a value for placeholder tensor 'X_8' with dtype float and shape [?,9]\n\t [[node X_8 (defined at C:\\Users\\huanyizhiyuan\\AppData\\Local\\Temp\\ipykernel_32524\\966860098.py:2) ]]\n\nOriginal stack trace for 'X_8':\n  File \"D:\\Anaconda\\envs\\tensor_test_1x\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"D:\\Anaconda\\envs\\tensor_test_1x\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"D:\\Anaconda\\envs\\tensor_test_1x\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n    app.launch_new_instance()\n  File \"D:\\Anaconda\\envs\\tensor_test_1x\\lib\\site-packages\\traitlets\\config\\application.py\", line 1043, in launch_instance\n    app.start()\n  File \"D:\\Anaconda\\envs\\tensor_test_1x\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n    self.io_loop.start()\n  File \"D:\\Anaconda\\envs\\tensor_test_1x\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n    self.asyncio_loop.run_forever()\n  File \"D:\\Anaconda\\envs\\tensor_test_1x\\lib\\asyncio\\base_events.py\", line 541, in run_forever\n    self._run_once()\n  File \"D:\\Anaconda\\envs\\tensor_test_1x\\lib\\asyncio\\base_events.py\", line 1786, in _run_once\n    handle._run()\n  File \"D:\\Anaconda\\envs\\tensor_test_1x\\lib\\asyncio\\events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"D:\\Anaconda\\envs\\tensor_test_1x\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n    await self.process_one()\n  File \"D:\\Anaconda\\envs\\tensor_test_1x\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n    await dispatch(*args)\n  File \"D:\\Anaconda\\envs\\tensor_test_1x\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n    await result\n  File \"D:\\Anaconda\\envs\\tensor_test_1x\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n    reply_content = await reply_content\n  File \"D:\\Anaconda\\envs\\tensor_test_1x\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 387, in do_execute\n    cell_id=cell_id,\n  File \"D:\\Anaconda\\envs\\tensor_test_1x\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"D:\\Anaconda\\envs\\tensor_test_1x\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2976, in run_cell\n    raw_cell, store_history, silent, shell_futures, cell_id\n  File \"D:\\Anaconda\\envs\\tensor_test_1x\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3030, in _run_cell\n    return runner(coro)\n  File \"D:\\Anaconda\\envs\\tensor_test_1x\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 78, in _pseudo_sync_runner\n    coro.send(None)\n  File \"D:\\Anaconda\\envs\\tensor_test_1x\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3258, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"D:\\Anaconda\\envs\\tensor_test_1x\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3473, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"D:\\Anaconda\\envs\\tensor_test_1x\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3553, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"C:\\Users\\huanyizhiyuan\\AppData\\Local\\Temp\\ipykernel_32524\\966860098.py\", line 2, in <module>\n    X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n  File \"D:\\Anaconda\\envs\\tensor_test_1x\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 2143, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"D:\\Anaconda\\envs\\tensor_test_1x\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 6261, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"D:\\Anaconda\\envs\\tensor_test_1x\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"D:\\Anaconda\\envs\\tensor_test_1x\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"D:\\Anaconda\\envs\\tensor_test_1x\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"D:\\Anaconda\\envs\\tensor_test_1x\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
     ]
    }
   ],
   "source": [
    "# 模型的保存\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())  # 保存运行过程\n",
    "            save_path = saver.save(sess, \"/my_model.ckpt\")\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "    save_path = saver.save(sess, \"/my_model_final.ckpt\")#保存最后的结果\n",
    "    \n",
    "# 模型的加载\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"/tmp/my_model_final.ckpt\")\n",
    "    best_theta_restored = theta.eval() \n",
    "    \n",
    "saver = tf.train.Saver({\"weights\":theta})\n",
    "\n",
    "\n",
    "reset_graph()\n",
    "saver = tf.train.import_meta_graph(\"/tmp/my_model_final.ckpt.meta\")\n",
    "theta = tf.get_default_graph().get_tensor_by_name(\"theta:0\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T11:08:39.513685200Z",
     "start_time": "2024-02-28T11:08:39.410603400Z"
    }
   },
   "id": "4daff7941f7dd428"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T11:04:01.970711600Z",
     "start_time": "2024-02-28T11:04:01.955616300Z"
    }
   },
   "id": "8d247d4d092f61ac"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "tensor_test_1x",
   "language": "python",
   "display_name": "tensor_test_1x"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
